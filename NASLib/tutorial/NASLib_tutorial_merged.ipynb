{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNilnGngasuy"
      },
      "source": [
        "#NASLib Hands-On\n",
        "\n",
        "##Welcome to the [NASLib](https://github.com/automl/NASLib) tutorial ðŸ“–\n",
        "\n",
        "Awesome to have you here!\n",
        "\n",
        "This notebook is part of the **NASLib Hands-On Session** for the [2nd AutoML Fall school](https://sites.google.com/view/automl-fall-school-2022). Before you start, please copy this notebook to your Google drive (`File` âž¡ `Save a Copy in Drive`).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8sT_Y2t3iVx"
      },
      "source": [
        "#[NASLib](https://github.com/automl/NASLib/) overview ðŸ—’\n",
        "NASLib is a modular and flexible framework created with the aim of providing a common codebase to the community to facilitate research on Neural Architecture Search (NAS). It offers high-level abstractions for designing and reusing search spaces, interfaces to benchmarks and evaluation pipelines, enabling the implementation and extension of several NAS methods with a few lines of code.\n",
        "\n",
        "![](https://raw.githubusercontent.com/automl/NASLib/Develop_copy/images/naslib-overall.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8ylWVm3Nic"
      },
      "source": [
        "This tutorial will focus on Neural Architecture Search and how NASLib can be leveraged to simplify it greatly.\n",
        "\n",
        "The tutorial is divided into 4 sections:\n",
        "1. [Setting up NASLib](#setup)\n",
        "2. [Search Spaces in NASLib](#search_spaces)\n",
        "3. [Optimizers in NASLib](#optimizers)\n",
        "4. [Zero Cost Predictors in NASLib](#zcps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x1lsktRjFR9"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "# 1. Setting Up NASLib ðŸ’»\n",
        "\n",
        "We begin by setting up NASLib. First we, clone the repository and install NASLib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlYZGxcOjMTN",
        "outputId": "a276e723-88f6-446c-9c72-fe2e989222b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'NASLib'...\n",
            "remote: Enumerating objects: 32063, done.\u001b[K\n",
            "remote: Counting objects: 100% (2032/2032), done.\u001b[K\n",
            "remote: Compressing objects: 100% (824/824), done.\u001b[K\n",
            "remote: Total 32063 (delta 1253), reused 1913 (delta 1156), pack-reused 30031\u001b[K\n",
            "Receiving objects: 100% (32063/32063), 549.33 MiB | 24.06 MiB/s, done.\n",
            "Resolving deltas: 100% (22647/22647), done.\n",
            "/content/NASLib\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 636 kB 5.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49 kB 5.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 280 kB 57.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.0 MB 59.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 166.7 MB 16 kB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 45 kB 3.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 40.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 634 kB 63.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75 kB 4.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.5 MB 34.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 187 kB 85.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.4 MB 59.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.7 MB 35.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42 kB 951 kB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 154 kB 57.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 349 kB 53.6 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133 kB 61.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77 kB 5.9 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.6 MB 65.1 MB/s \n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pybnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorwatch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pydotz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Error parsing requirements for lightgbm: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/lightgbm-2.2.3.dist-info/METADATA'\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.1.2 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "confection 0.0.2 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#Setup NASLib\n",
        "%cd /content\n",
        "%rm -rf NASLib\n",
        "!git clone -b Develop_copy https://github.com/automl/NASLib/\n",
        "%cd /content/NASLib\n",
        "!pip install -e . --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UEsFrMEtUrJ"
      },
      "source": [
        "You can ignore the pip errors relating to `typing-extensions`. Run the next cell to see if NASLib has been successfully installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1l1Vi8dotQBF"
      },
      "outputs": [],
      "source": [
        "import naslib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUVBPcYzr_8a"
      },
      "source": [
        "Next, we download a few datasets and benchmark files that we shall use in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDUb6gJeon9n",
        "outputId": "bacb7b63-1249-480e-a1a2-a4de8854bc8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/NASLib\n",
            "dataset = cifar10\n",
            "search_space = nb201\n",
            "cifar10 exists\n",
            "dataset = cifar100\n",
            "search_space = nb201\n",
            "cifar100 exists\n",
            "dataset = ImageNet16-120\n",
            "search_space = nb201\n",
            "Imagenet16 exists\n",
            "Imagenet16 data exits\n",
            "dataset = cifar10\n",
            "search_space = nb301\n",
            "nb301 full training pickle exists\n",
            "search_space = nb201\n",
            "nb201 file exist\n"
          ]
        }
      ],
      "source": [
        "# Download benchmarks\n",
        "%cd /content/NASLib\n",
        "!source /content/NASLib/scripts/bash_scripts/download_benchmarks.sh nb201 cifar10\n",
        "!source /content/NASLib/scripts/bash_scripts/download_benchmarks.sh nb201 cifar100\n",
        "!source /content/NASLib/scripts/bash_scripts/download_benchmarks.sh nb201 ImageNet16-120\n",
        "!source /content/NASLib/scripts/bash_scripts/download_benchmarks.sh nb301 cifar10\n",
        "!source scripts/zc/bash_scripts/download_nbs_zero.sh nb201\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fin64h_slQMe"
      },
      "source": [
        "If downloading the datasets using the script failed you can also download the data folder from [here](https://drive.google.com/file/d/1xjs6y2cf_mXKFjrT1auAynLMg48onQ5R/view?usp=sharing). If you need assistance, please let us know!\n",
        "Download and unzip the data folder and place its contents in `NASLib/naslib/data` using the commands below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UKs1ezlnEyB",
        "outputId": "a8b981e9-d236-47eb-dfdf-f8fcc6b49bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xjs6y2cf_mXKFjrT1auAynLMg48onQ5R\n",
            "To: /content/data.zip\n",
            "100% 383M/383M [00:01<00:00, 281MB/s]\n",
            "Archive:  data.zip\n",
            "  inflating: data/train_data_batch_1  \n",
            "  inflating: __MACOSX/data/._train_data_batch_1  \n",
            "  inflating: data/nb301_full_training.pickle  \n",
            "  inflating: __MACOSX/data/._nb301_full_training.pickle  \n",
            "  inflating: data/nb201_ImageNet16_full_training.pickle  \n",
            "  inflating: __MACOSX/data/._nb201_ImageNet16_full_training.pickle  \n",
            "  inflating: data/nb201_cifar10_full_training.pickle  \n",
            "  inflating: __MACOSX/data/._nb201_cifar10_full_training.pickle  \n",
            "  inflating: data/zc_nasbench201.json  \n",
            "  inflating: __MACOSX/data/._zc_nasbench201.json  \n",
            "  inflating: data/nb201_cifar100_full_training.pickle  \n",
            "  inflating: __MACOSX/data/._nb201_cifar100_full_training.pickle  \n",
            "  inflating: data/val_data           \n",
            "  inflating: __MACOSX/data/._val_data  \n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  data.zip  \u001b[01;34m__MACOSX\u001b[0m/  \u001b[01;34mNASLib\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n",
            "/content/NASLib\n"
          ]
        }
      ],
      "source": [
        "#Caution: Run this cell only if downloading data using the script above fails\n",
        "%cd /content\n",
        "!gdown 1xjs6y2cf_mXKFjrT1auAynLMg48onQ5R\n",
        "!unzip -o data.zip \n",
        "%ls\n",
        "%mv data/* /content/NASLib/naslib/data/\n",
        "%cd /content/NASLib/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYzxD6H5shyc"
      },
      "source": [
        "With that, setup is complete. We can now move on to the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0DG0dOFvsSP"
      },
      "source": [
        "<a name=\"search_spaces\"></a>\n",
        "# 2. Search Spaces in NASLib\n",
        "### What is NAS Search Space?\n",
        "The search space defines which architectures can be represented using a choice of operations and topologies. Incorporating prior knowledge about typical properties of architectures well-suited for a task can reduce the size of the search space and simplify the search.However, this also introduces a human bias, which may prevent finding\n",
        "novel architectural building blocks that go beyond the current human knowledge.\n",
        "\n",
        "Let us consider the [NAS-Bench-201](https://arxiv.org/pdf/2001.00326.pdf) search space for example. This search space consists of multiple stacked cells. Each cell is a directed acyclic graph, and each operation selected from a predefined set of possible choices. \n",
        "\n",
        "![](https://drive.google.com/uc?id=1Uw6InTaEWaE2gwgct6ztkLudEz7X3W2y)\n",
        "\n",
        "### What is a NAS Benchmark?\n",
        "A NAS benchmark consits of the true or estimated performance of architectures from a NAS Search space each of which are evaluated on a fixed train-val-test aplit of the dataset and a fixed training pipeline. There are primarily two types of NAS benchmarks: a ***surrogate NAS benchmark*** and a ***tabular*** NAS benchmark. \n",
        "\n",
        "A surrogate benchmark uses a surrogate model (eg: a neural network) to predict the performance of an architecture from the search space. On the other hand a tabular benchmark queries the actual performance of an architecture trained upto convergence.\n",
        "\n",
        "### Why do we need NAS Benchmarks?\n",
        "\n",
        "1.   Neural architecture search often requires large computational resources. Evaluating a single model required one full training of a network\n",
        "2.   Reproducibility in NAS research \n",
        "\n",
        "###NASLib supports multiple search spaces and benchmarks like:\n",
        "\n",
        "1. [NAS-Bench-101](https://arxiv.org/pdf/1902.09635.pdf)\n",
        "2. [NAS-Bench-201](https://arxiv.org/pdf/2001.00326.pdf)\n",
        "3. [NAS-Bench-301](https://arxiv.org/pdf/2008.09777.pdf)\n",
        "4. [TransNAS-Bench-101](https://arxiv.org/pdf/2105.11871.pdf)\n",
        "\n",
        "### With a unified API, NASLib allows you to trivially perform several operations, such as:\n",
        "\n",
        "1. Sample a random model from the search space, which you can use as an actual PyTorch model\n",
        "2. Query the Benchmark APIs of models for metrics such as training and validation accuracies\n",
        "3. Convert between different representations of a given architecture\n",
        "4. Mutate an architecture\n",
        "\n",
        "In the next part of the tutorial, we walk through each of these functionalities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38Rw8zoHobaW",
        "outputId": "cd2f19df-0e23-4464-9ba1-7e2def7e6370"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 8.1.2. Several security issues (CVE-2021-27921, CVE-2021-25290, CVE-2021-25291, CVE-2021-25293, and more) have been fixed in pillow 8.1.2 or higher. We recommend to upgrade this library.\n",
            "  from .collection import imread_collection_wrapper\n"
          ]
        }
      ],
      "source": [
        "# Import the search space\n",
        "# We begin by importing only the NAS-Bench-201 Search Space\n",
        "from naslib.search_spaces import NasBench201SearchSpace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "mupGqiYP6sHj"
      },
      "outputs": [],
      "source": [
        "# Create a new search space object.\n",
        "# This object doesn't have an architecture assigned to it yet - it represents the entire search space\n",
        "graph = NasBench201SearchSpace(n_classes=10) # CIFAR-10 dataset requires 10 classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0QBgM518odz",
        "outputId": "837973a8-43c3-45c4-af53-59821c7ed782"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0, 0, 3, 2, 1, 2)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample a random architecture\n",
        "# You can call this method only once on any instance\n",
        "graph.sample_random_architecture()\n",
        "graph.parse()\n",
        "\n",
        "# Get the NASLib representation \n",
        "#(eg: operation index chosen at every edge) of the sampled architecture\n",
        "graph.get_hash()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LEgt8uD8seL",
        "outputId": "170185c4-a039-4118-8ab3-ad1dd4d76fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the logits: torch.Size([5, 10])\n"
          ]
        }
      ],
      "source": [
        "# This graph is now a NAS-Bench-201 model, which can be used for training\n",
        "# Forward pass some dummy data through it to see it in action\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.randn(5, 3, 32, 32) # (Batch_size, Num_channels, Height, Width)\n",
        "\n",
        "logits = graph(x)\n",
        "\n",
        "print('Shape of the logits:', logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l6VYYjdy8v4f",
        "outputId": "fbd1339d-b23d-49ae-9410-c8b9a2c32830"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|skip_connect~0|+|skip_connect~0|nor_conv_3x3~1|+|nor_conv_1x1~0|none~1|nor_conv_3x3~2|'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import code to convert NASLib graph to the original NAS-Bench-201 representation\n",
        "from naslib.search_spaces.nasbench201.conversions import convert_naslib_to_str as convert_naslib_nb201_to_str\n",
        "\n",
        "# Get the string representation of this model, that the original authors of NAS-Bench-201 used\n",
        "convert_naslib_nb201_to_str(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZwyk6jX8ysb",
        "outputId": "e78c19da-e31c-44f9-da80-e53b71a9136e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parent graph: (0, 0, 3, 2, 1, 2)\n",
            "Child graph : (4, 0, 3, 2, 1, 2)\n"
          ]
        }
      ],
      "source": [
        "# Mutating an architecture\n",
        "# First, create a new child_graph\n",
        "child_graph = NasBench201SearchSpace(n_classes=10)\n",
        "\n",
        "# Call mutate on the child graph by passing the parent graph to it\n",
        "child_graph.mutate(parent=graph)\n",
        "\n",
        "# See the parent and child graph representations. Which edge was mutated?\n",
        "print(f'Parent graph: {graph.get_hash()}')\n",
        "print(f'Child graph : {child_graph.get_hash()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rLf2sPC--Glk"
      },
      "outputs": [],
      "source": [
        "# Now, let's load the queryable tabular NAS-Bench-201 API\n",
        "# This API has the training metrics of all the 15625 models in the search space\n",
        "# such as train and validation accuracies/losses at every epoch\n",
        "\n",
        "from naslib.utils import get_dataset_api\n",
        "benchmark_api = get_dataset_api(search_space='nasbench201', dataset='cifar10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crouZBso95zc",
        "outputId": "f59e8618-1220-4113-ef21-3b3779d976a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance of parent model\n",
            "Train accuracy: 99.97%\n",
            "Validation accuracy: 89.27%\n"
          ]
        }
      ],
      "source": [
        "# With the NAS-Bench-201 API, we can now query, say, the validation performance of any NB201 model\n",
        "# Without it, we would have to train the model from scratch to get this information\n",
        "\n",
        "# First, import the Metric enum\n",
        "from naslib.search_spaces.core import Metric\n",
        "\n",
        "# Metric has, among others, these values:\n",
        "# Metric.TRAIN_ACCURACY\n",
        "# Metric.VAL_ACCURACY\n",
        "# Metric.TRAIN_LOSS\n",
        "# Metric.TEST_LOSS\n",
        "# Metric.TRAIN_TIME\n",
        "\n",
        "train_acc_parent = graph.query(metric=Metric.TRAIN_ACCURACY, dataset='cifar10', dataset_api=benchmark_api)\n",
        "val_acc_parent = graph.query(metric=Metric.VAL_ACCURACY, dataset='cifar10', dataset_api=benchmark_api)\n",
        "\n",
        "print('Performance of parent model')\n",
        "print(f'Train accuracy: {train_acc_parent:.2f}%')\n",
        "print(f'Validation accuracy: {val_acc_parent:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCkD1qVE4KFl"
      },
      "source": [
        "#Task 1: Play with NAS-Bench-301 (10 minutes)\n",
        "\n",
        "Now that you are familiar with the operations search spaces in NASLib, we can attempt our first task. In this task, you should do the following:\n",
        "\n",
        "1. Sample a random NAS-Bench-301 model\n",
        "2. Get the NASLib and genotype representations of the model\n",
        "3. Query the predicted performance of the model (loading the NB301 benchmark API might take some time)\n",
        "4. Mutate the model\n",
        "5. Get the NASLib and genotype representations of the model\n",
        "6. Query the predicted performance of the child\n",
        "\n",
        "All the modules you require are already imported for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lvxWRWKU_UTd"
      },
      "outputs": [],
      "source": [
        "from naslib.search_spaces import NasBench301SearchSpace\n",
        "from naslib.search_spaces.nasbench301.conversions import convert_naslib_to_genotype as convert_naslib_nb301_to_genotype\n",
        "\n",
        "########## START TODO ############\n",
        "# TODO: \n",
        "# 1. Sample a random NAS-Bench-301 model\n",
        "# 2. Get the NASLib and genotype representations of the model\n",
        "# 3. Query the predicted performance of the model (loading the NB301 benchmark API might take some time)\n",
        "# 4. Mutate the model\n",
        "# 5. Get the NASLib and genotype representations of the model\n",
        "# 6. Query the predicted performance of the child\n",
        "\n",
        "\n",
        "\n",
        "########## END TODO ##############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWB4xUZr1wu4"
      },
      "source": [
        "<a name=\"optimizers\"></a>\n",
        "## 3. NAS Optimizers ðŸ“ˆ\n",
        "\n",
        "## What is a NAS Optimizer?\n",
        "The search strategy details how to explore the search space to find\n",
        "well-performing architectures quickly.\n",
        "\n",
        "![](https://drive.google.com/uc?id=1e7a4LI-f7embNqUq23FwlysP6mQu63nx)\n",
        "\n",
        "Given the different NAS benchmarks we now want to search for the optimal architectures from the search spaces. NASLib provides a uniform api to use multiple optimizers across different benchmarks. One could easily write their own optimizer and evaluate it across the different NAS benchmarks. \n",
        "\n",
        "## What is a performance predictor?\n",
        "Performance Estimation in NAS refers to estimating the performance of a given architecture. One trivial example of this is training a candidate architecture to convergence, but this can get very expensive. Much recent research therefore focuses on developing methods that reduce the cost of these performance estimations. A [performance predictor](https://arxiv.org/pdf/2104.01177.pdf) is defined generally as any function which predicts the final accuracy or ranking of architectures, without fully training the architectures (eg: xgboost)\n",
        "\n",
        "Some of the optimizers supported in NASLib are:\n",
        "\n",
        "\n",
        "1.   [Random search](https://arxiv.org/pdf/1902.07638.pdf)\n",
        "2.   [Local Search](https://arxiv.org/pdf/2004.08996.pdf)\n",
        "3.   [Regularized Evolution](https://arxiv.org/pdf/1802.01548.)\n",
        "4.   [BANANAS](https://arxiv.org/pdf/1910.11858.pdf)\n",
        "\n",
        "In this section of the tutorial we will walk through the following components:\n",
        "\n",
        "\n",
        "1. Searching for an architecture using [Random search (RS)](https://arxiv.org/pdf/1902.07638.pdf) on different NAS benchmarks.\n",
        "2. Searching for an architecture using [Regularized Evolution (RE)](https://arxiv.org/pdf/1802.01548.pdf) and comparing it with RS\n",
        "3. Searching for an architecture using [BANANAS](https://arxiv.org/pdf/1910.11858.pdf) and comparing with RE and RS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0rxz8RRaAOef"
      },
      "outputs": [],
      "source": [
        "# The functions below setup and run the search experiments\n",
        "\n",
        "def update_config(config, optimizer_type, search_space_type, dataset, seed):\n",
        "    # Dataset being used\n",
        "    config.dataset = dataset\n",
        "    \n",
        "    # Directory to which the results/logs will be saved\n",
        "    config.save = f\"runs/{optimizer_type.__name__}/{search_space_type.__name__}/{dataset}/{seed}\"\n",
        "    \n",
        "    # Seed used during search phase of the optimizer\n",
        "    config.search.seed = seed\n",
        "    \n",
        "def run_optimizer(optimizer_type, search_space_type, dataset, config, seed): \n",
        "    # Update the config\n",
        "    update_config(config, optimizer_type, search_space_type, dataset, seed)\n",
        "\n",
        "    # Make the results directories\n",
        "    os.makedirs(config.save + '/search', exist_ok=True)\n",
        "    os.makedirs(config.save + '/eval', exist_ok=True)\n",
        "\n",
        "    # Set up the loggers\n",
        "    logger = setup_logger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "     # See the config\n",
        "    logger.info(f'Configuration is \\n{config}')\n",
        "\n",
        "    # Set up the seed\n",
        "    utils.set_seed(seed)\n",
        "    # Instantiate the search space\n",
        "    n_classes = {\n",
        "        'cifar10': 10,\n",
        "        'cifar100': 100,\n",
        "        'ImageNet16-120': 120\n",
        "    }\n",
        "    search_space = search_space_type(n_classes=n_classes[dataset])\n",
        "    search_space.instantiate_model = False\n",
        "    # Get the benchmark API\n",
        "    logger.info('Loading Benchmark API')\n",
        "    dataset_api = get_dataset_api(search_space.get_type(), dataset)\n",
        "    \n",
        "    # Instantiate the optimizer and adapat the search space to the optimizer\n",
        "    optimizer = optimizer_type(config)\n",
        "    optimizer.adapt_search_space(search_space, dataset_api=dataset_api)\n",
        "\n",
        "    # Create a Trainer\n",
        "    trainer = Trainer(optimizer, config)\n",
        "\n",
        "    # Perform the search\n",
        "    trainer.search(report_incumbent=False)\n",
        "\n",
        "    # Get the results of the search\n",
        "    search_trajectory = trainer.search_trajectory\n",
        "    print('Train accuracies:', search_trajectory.train_acc)\n",
        "    print('Validation accuracies:', search_trajectory.valid_acc)\n",
        "    # Get the validation performance of the best model found in the search phase\n",
        "    best_model_val_acc = trainer.evaluate(dataset_api=dataset_api, metric=Metric.VAL_ACCURACY)\n",
        "    best_model_val_acc\n",
        "\n",
        "    best_model = optimizer.get_final_architecture()\n",
        "\n",
        "    return search_trajectory, best_model, best_model_val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeL5A3MeXAYm",
        "outputId": "a6f9a9e0-950b-4852-82f5-304a9b68ea5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "# Defining plotting utils\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
        "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
        "from matplotlib.ticker import FormatStrFormatter\n",
        "\n",
        "# plot performance vs runtime\n",
        "\n",
        "# didn't run them long enough to do logspace here. (These experiments took surprisingly long to run)\n",
        "plt.rcParams['axes.grid'] = True\n",
        "plt.rcParams['grid.linestyle'] = 'dotted'\n",
        "# set up colors and plot markings\n",
        "defaults = [(0.12156862745098039, 0.4666666666666667, 0.7058823529411765),\n",
        "            (1.0, 0.4980392156862745, 0.054901960784313725),\n",
        "            (0.17254901960784313, 0.6274509803921569, 0.17254901960784313),\n",
        "            (0.8392156862745098, 0.15294117647058825, 0.1568627450980392),\n",
        "            (0.5803921568627451, 0.403921568627451, 0.7411764705882353),\n",
        "            (0.5490196078431373, 0.33725490196078434, 0.29411764705882354),\n",
        "            (0.8901960784313725, 0.4666666666666667, 0.7607843137254902),\n",
        "            (0.4980392156862745, 0.4980392156862745, 0.4980392156862745),\n",
        "            (0.7372549019607844, 0.7411764705882353, 0.13333333333333333),\n",
        "            (0.09019607843137255, 0.7450980392156863, 0.8117647058823529)]\n",
        "\n",
        "# goes up to 24\n",
        "c_max = 9\n",
        "colors = [*defaults[:c_max], *defaults[:c_max], *defaults[:c_max]]\n",
        "fmts = [*['-'] * c_max, *['--'] * c_max, *[':'] * c_max]\n",
        "markers = [*['^'] * c_max, *['v'] * c_max, *['o'] * c_max]\n",
        "\n",
        "\n",
        "def get_results(results, metric='valid_acc', dataset='cifar10', ug=False):\n",
        "    output = []\n",
        "    time = []\n",
        "    for result in results:\n",
        "        val_acc = result['valid_acc']\n",
        "        surr_time = np.array(result['runtime'])\n",
        "        if ug:\n",
        "            runtime = 200 * np.array(result['train_time']) + surr_time\n",
        "        else:\n",
        "            runtime = np.array(result['train_time']) + surr_time\n",
        "        val_err = [100 - x for x in val_acc]\n",
        "        val_incumbent = [\n",
        "            min(val_err[:epoch]) for epoch in range(1,\n",
        "                                                    len(val_err) + 1)\n",
        "        ]\n",
        "        runtime = [\n",
        "            sum(runtime[:epoch]) for epoch in range(1,\n",
        "                                                    len(runtime) + 1)\n",
        "        ]\n",
        "        if metric == 'valid_acc':\n",
        "            incumbent = val_incumbent\n",
        "        elif metric == 'test_acc':\n",
        "            test_err = [100 - x for x in result['test_acc']]\n",
        "            inc_idx, best, best_idx = [], np.inf, 0\n",
        "            for i, err in enumerate(val_err):\n",
        "                if err < best:\n",
        "                    best, best_idx = err, i\n",
        "                    inc_idx.append(best_idx)\n",
        "                    incumbent = [test_err[idx] for idx in inc_idx]\n",
        "        output.append(incumbent)\n",
        "        time.append(runtime)\n",
        "    output = np.array(output)\n",
        "    time = np.array(runtime)\n",
        "    mean = np.mean(output, axis=0)\n",
        "    std = np.std(output, axis=0)\n",
        "    std_error = np.sqrt(\n",
        "        np.var(output, axis=0, ddof=1) / np.asarray(output).shape[0])\n",
        "    return mean, std, std_error, time\n",
        "\n",
        "\n",
        "def plot_optimizers(trajectories,benchmark):\n",
        "    plot_zoomed = False\n",
        "    plot_sem = True\n",
        "    fig, ax = plt.subplots(figsize=[10, 5])\n",
        "    if plot_zoomed:\n",
        "        sub_axes = plt.axes([.6, .6, .25, .25])\n",
        "    i = 0\n",
        "    for optimizer in trajectories.keys():\n",
        "        mean, std, std_error, time = get_results(trajectories[optimizer])\n",
        "        results_dict = {}\n",
        "        results_dict = {\n",
        "            'label': optimizer,\n",
        "            'mean': mean,\n",
        "            'runtime': time,\n",
        "            'std_error': std_error\n",
        "        }\n",
        "        mean = results_dict['mean']\n",
        "        #print(mean.shape)\n",
        "        x = results_dict['runtime']\n",
        "        #print(x.shape)\n",
        "        sem = results_dict['std_error']\n",
        "        #print(sem.shape)\n",
        "        label = results_dict[\"label\"]\n",
        "        ax.plot(x, mean, label=label, color=colors[i], linestyle=fmts[i])\n",
        "        ax.fill_between(x,\n",
        "                        mean - 1 * sem,\n",
        "                        mean + 1 * sem,\n",
        "                        color=colors[i],\n",
        "                        alpha=0.2)\n",
        "        i = i + 1\n",
        "    ax.yaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
        "    ax.set_xscale('log')\n",
        "    ax.legend(loc=(1.04, 0))\n",
        "    ax.set_xlabel('Runtime [s] (simulated)')\n",
        "    ax.set_ylabel('Test error (%)')\n",
        "    ax.grid(True, which=\"both\", ls=\"-\", alpha=.5)\n",
        "    ax.set_title('Test error vs. train time on '+benchmark)\n",
        "\n",
        "\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vSu7eJkOuOA"
      },
      "source": [
        "### Random Search\n",
        "1. Sample an architecture for every epoch and compute its validation error \n",
        "2. Select the architecture with the lowest validation error and return its test error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "I6zJDWztBRq1"
      },
      "outputs": [],
      "source": [
        "#initialize trajectories for RS\n",
        "trajectories_rs = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "LfiI8b6eRxO1"
      },
      "outputs": [],
      "source": [
        "#imports we will use throught this section\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# import the Trainer used to run the optimizer on a given search space\n",
        "from naslib.defaults.trainer import Trainer\n",
        "# import the optimizers\n",
        "from naslib.optimizers import (\n",
        "    RandomSearch,\n",
        "    RegularizedEvolution,\n",
        "    Bananas\n",
        ")\n",
        "# import the search spaces\n",
        "from naslib.search_spaces import (\n",
        "    NasBench101SearchSpace,\n",
        "    NasBench201SearchSpace,\n",
        "    NasBench301SearchSpace,\n",
        ")\n",
        "\n",
        "from naslib.search_spaces.core.query_metrics import Metric\n",
        "from naslib import utils\n",
        "from naslib.utils import get_dataset_api\n",
        "from naslib.utils.logging import setup_logger\n",
        "\n",
        "from fvcore.common.config import CfgNode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvhKQVHFAaGx",
        "outputId": "59fa6f39-0c18-4186-aee2-1b44a03c9e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/08 11:21:52 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs/RandomSearch/NasBench201SearchSpace/cifar10/333\n",
            "search:\n",
            "  checkpoint_freq: 200\n",
            "  epochs: 100\n",
            "  fidelity: -1\n",
            "  population_size: 30\n",
            "  sample_size: 10\n",
            "  seed: 333\n",
            "\u001b[32m[10/08 11:21:52 naslib]: \u001b[0mLoading Benchmark API\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mBeginning search\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 99.54800, Validation accuracy: 83.83000\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 72.61200, Validation accuracy: 66.22000\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 99.91200, Validation accuracy: 88.09000\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 99.88400, Validation accuracy: 88.68000\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 99.92000, Validation accuracy: 89.14000\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy: 99.98800, Validation accuracy: 90.01000\n",
            "\u001b[32m[10/08 11:21:54 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy: 97.94400, Validation accuracy: 85.91000\n",
            "\u001b[32m[10/08 11:21:55 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy: 89.03200, Validation accuracy: 81.87000\n",
            "\u001b[32m[10/08 11:21:55 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy: 72.39200, Validation accuracy: 66.30000\n",
            "\u001b[32m[10/08 11:21:55 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy: 99.52800, Validation accuracy: 87.35000\n",
            "\u001b[32m[10/08 11:21:55 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy: 91.91600, Validation accuracy: 83.42000\n",
            "\u001b[32m[10/08 11:21:55 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy: 99.98000, Validation accuracy: 87.95000\n",
            "\u001b[32m[10/08 11:21:55 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy: 99.95200, Validation accuracy: 89.08000\n",
            "\u001b[32m[10/08 11:21:55 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy: 99.33600, Validation accuracy: 85.22000\n",
            "\u001b[32m[10/08 11:21:56 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy: 96.72400, Validation accuracy: 85.97000\n",
            "\u001b[32m[10/08 11:21:56 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy: 99.68400, Validation accuracy: 87.61000\n",
            "\u001b[32m[10/08 11:21:56 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy: 99.75200, Validation accuracy: 87.90000\n",
            "\u001b[32m[10/08 11:21:56 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy: 99.93200, Validation accuracy: 87.75000\n",
            "\u001b[32m[10/08 11:21:56 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy: 99.94800, Validation accuracy: 87.17000\n",
            "\u001b[32m[10/08 11:21:56 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy: 99.98400, Validation accuracy: 90.90000\n",
            "\u001b[32m[10/08 11:21:56 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy: 91.22000, Validation accuracy: 84.73000\n",
            "\u001b[32m[10/08 11:21:57 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy: 99.98800, Validation accuracy: 89.33000\n",
            "\u001b[32m[10/08 11:21:57 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy: 99.03600, Validation accuracy: 86.10000\n",
            "\u001b[32m[10/08 11:21:58 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy: 80.56400, Validation accuracy: 70.60000\n",
            "\u001b[32m[10/08 11:21:58 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy: 99.84400, Validation accuracy: 83.95000\n",
            "\u001b[32m[10/08 11:21:58 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy: 99.71200, Validation accuracy: 85.71000\n",
            "\u001b[32m[10/08 11:21:58 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy: 99.95600, Validation accuracy: 89.23000\n",
            "\u001b[32m[10/08 11:21:58 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy: 93.84800, Validation accuracy: 84.40000\n",
            "\u001b[32m[10/08 11:21:58 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy: 99.96800, Validation accuracy: 90.15000\n",
            "\u001b[32m[10/08 11:21:58 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy: 99.62000, Validation accuracy: 86.15000\n",
            "\u001b[32m[10/08 11:21:59 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy: 99.88800, Validation accuracy: 88.82000\n",
            "\u001b[32m[10/08 11:21:59 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy: 99.96400, Validation accuracy: 88.33000\n",
            "\u001b[32m[10/08 11:21:59 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy: 99.97200, Validation accuracy: 90.10000\n",
            "\u001b[32m[10/08 11:22:00 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy: 99.96400, Validation accuracy: 88.78000\n",
            "\u001b[32m[10/08 11:22:00 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy: 80.54000, Validation accuracy: 70.81000\n",
            "\u001b[32m[10/08 11:22:00 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy: 99.97600, Validation accuracy: 89.24000\n",
            "\u001b[32m[10/08 11:22:00 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy: 99.94800, Validation accuracy: 89.45000\n",
            "\u001b[32m[10/08 11:22:00 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy: 98.33200, Validation accuracy: 86.13000\n",
            "\u001b[32m[10/08 11:22:01 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy: 90.07600, Validation accuracy: 75.50000\n",
            "\u001b[32m[10/08 11:22:01 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy: 99.26800, Validation accuracy: 86.98000\n",
            "\u001b[32m[10/08 11:22:01 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy: 99.93200, Validation accuracy: 89.72000\n",
            "\u001b[32m[10/08 11:22:01 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy: 83.03200, Validation accuracy: 78.51000\n",
            "\u001b[32m[10/08 11:22:01 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy: 99.96800, Validation accuracy: 89.14000\n",
            "\u001b[32m[10/08 11:22:01 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy: 98.34400, Validation accuracy: 84.97000\n",
            "\u001b[32m[10/08 11:22:01 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy: 98.59200, Validation accuracy: 85.20000\n",
            "\u001b[32m[10/08 11:22:02 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy: 99.95200, Validation accuracy: 88.90000\n",
            "\u001b[32m[10/08 11:22:02 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy: 99.89200, Validation accuracy: 86.81000\n",
            "\u001b[32m[10/08 11:22:03 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy: 99.93600, Validation accuracy: 89.10000\n",
            "\u001b[32m[10/08 11:22:03 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy: 99.92000, Validation accuracy: 88.74000\n",
            "\u001b[32m[10/08 11:22:03 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy: 99.97200, Validation accuracy: 89.65000\n",
            "\u001b[32m[10/08 11:22:03 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy: 75.53600, Validation accuracy: 72.63000\n",
            "\u001b[32m[10/08 11:22:03 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy: 99.93600, Validation accuracy: 88.75000\n",
            "\u001b[32m[10/08 11:22:04 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy: 99.88800, Validation accuracy: 88.93000\n",
            "\u001b[32m[10/08 11:22:04 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy: 99.88400, Validation accuracy: 88.96000\n",
            "\u001b[32m[10/08 11:22:04 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy: 93.16000, Validation accuracy: 83.78000\n",
            "\u001b[32m[10/08 11:22:04 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy: 80.89600, Validation accuracy: 71.82000\n",
            "\u001b[32m[10/08 11:22:04 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy: 96.33600, Validation accuracy: 85.62000\n",
            "\u001b[32m[10/08 11:22:04 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy: 91.50400, Validation accuracy: 82.99000\n",
            "\u001b[32m[10/08 11:22:04 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy: 99.88000, Validation accuracy: 89.00000\n",
            "\u001b[32m[10/08 11:22:05 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy: 81.36800, Validation accuracy: 71.48000\n",
            "\u001b[32m[10/08 11:22:05 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy: 75.50000, Validation accuracy: 72.65000\n",
            "\u001b[32m[10/08 11:22:05 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy: 98.07200, Validation accuracy: 84.94000\n",
            "\u001b[32m[10/08 11:22:05 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy: 83.36800, Validation accuracy: 78.16000\n",
            "\u001b[32m[10/08 11:22:05 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy: 99.84800, Validation accuracy: 88.09000\n",
            "\u001b[32m[10/08 11:22:06 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy: 99.87600, Validation accuracy: 88.85000\n",
            "\u001b[32m[10/08 11:22:07 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy: 99.87600, Validation accuracy: 88.71000\n",
            "\u001b[32m[10/08 11:22:07 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy: 99.90400, Validation accuracy: 88.39000\n",
            "\u001b[32m[10/08 11:22:07 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy: 99.92800, Validation accuracy: 88.91000\n",
            "\u001b[32m[10/08 11:22:07 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy: 34.36800, Validation accuracy: 33.86000\n",
            "\u001b[32m[10/08 11:22:07 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy: 71.92400, Validation accuracy: 66.50000\n",
            "\u001b[32m[10/08 11:22:07 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy: 99.90000, Validation accuracy: 88.01000\n",
            "\u001b[32m[10/08 11:22:07 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy: 99.46400, Validation accuracy: 86.21000\n",
            "\u001b[32m[10/08 11:22:08 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy: 99.76400, Validation accuracy: 85.95000\n",
            "\u001b[32m[10/08 11:22:08 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy: 99.96400, Validation accuracy: 88.72000\n",
            "\u001b[32m[10/08 11:22:08 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy: 99.86800, Validation accuracy: 86.01000\n",
            "\u001b[32m[10/08 11:22:08 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy: 99.60800, Validation accuracy: 88.26000\n",
            "\u001b[32m[10/08 11:22:08 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy: 81.52400, Validation accuracy: 72.35000\n",
            "\u001b[32m[10/08 11:22:08 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy: 99.92400, Validation accuracy: 88.59000\n",
            "\u001b[32m[10/08 11:22:09 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy: 97.80800, Validation accuracy: 85.86000\n",
            "\u001b[32m[10/08 11:22:09 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy: 99.95200, Validation accuracy: 88.68000\n",
            "\u001b[32m[10/08 11:22:09 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy: 99.95600, Validation accuracy: 88.94000\n",
            "\u001b[32m[10/08 11:22:09 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy: 76.46800, Validation accuracy: 71.43000\n",
            "\u001b[32m[10/08 11:22:09 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy: 90.96800, Validation accuracy: 84.01000\n",
            "\u001b[32m[10/08 11:22:10 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy: 99.98800, Validation accuracy: 90.33000\n",
            "\u001b[32m[10/08 11:22:10 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy: 99.96400, Validation accuracy: 89.32000\n",
            "\u001b[32m[10/08 11:22:12 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy: 92.88000, Validation accuracy: 78.58000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy: 99.00800, Validation accuracy: 79.69000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy: 99.96400, Validation accuracy: 88.91000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy: 99.89200, Validation accuracy: 87.98000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy: 99.96400, Validation accuracy: 88.95000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy: 99.94800, Validation accuracy: 88.98000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy: 99.94400, Validation accuracy: 89.93000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy: 99.98800, Validation accuracy: 90.56000\n",
            "\u001b[32m[10/08 11:22:13 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy: 99.94000, Validation accuracy: 88.04000\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy: 99.93600, Validation accuracy: 88.18000\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy: 99.96000, Validation accuracy: 89.18000\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy: 98.53600, Validation accuracy: 84.03000\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy: 99.98400, Validation accuracy: 90.95000\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy: 92.82800, Validation accuracy: 79.55000\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy: 99.92000, Validation accuracy: 89.51000\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [99.548, 72.61199998291016, 99.912, 99.88400000244141, 99.92, 99.988, 97.94400000732422, 89.03200001220704, 72.39200001953125, 99.528, 91.91600001953125, 99.98, 99.952, 99.336, 96.72400001464844, 99.684, 99.75200000488282, 99.932, 99.948, 99.984, 91.21999999511719, 99.988, 99.03600000244141, 80.56399998779297, 99.84400000488282, 99.71200000488281, 99.956, 93.84799998046876, 99.968, 99.62, 99.888, 99.964, 99.972, 99.964, 80.54000000976562, 99.97600000244141, 99.948, 98.332, 90.07599999023438, 99.26800000244141, 99.932, 83.03200001220704, 99.968, 98.34400000732421, 98.59200000732422, 99.952, 99.892, 99.9360000024414, 99.92000000244141, 99.972, 75.53599999023437, 99.936, 99.888, 99.884, 93.15999998291015, 80.896, 96.33600000732422, 91.50399999023438, 99.88, 81.36800001464844, 75.50000000976563, 98.07200000976563, 83.36799998535156, 99.848, 99.876, 99.876, 99.9040000024414, 99.928, 34.368, 71.92399999023438, 99.9, 99.46400000244141, 99.76400000732421, 99.964, 99.86800000488282, 99.608, 81.52399997558594, 99.924, 97.80800000976562, 99.952, 99.956, 76.468, 90.96799999023438, 99.988, 99.964, 92.87999997802734, 99.00800000732421, 99.964, 99.892, 99.964, 99.948, 99.944, 99.988, 99.94, 99.936, 99.96, 98.53600001220703, 99.984, 92.82799999267579, 99.92]\n",
            "Validation accuracies: [83.83, 66.22, 88.09, 88.68, 89.14, 90.01, 85.91, 81.87, 66.3, 87.35, 83.42, 87.95, 89.08, 85.22, 85.97, 87.61, 87.9, 87.75, 87.17, 90.9, 84.73, 89.33, 86.1, 70.6, 83.95, 85.71, 89.23, 84.4, 90.15, 86.15, 88.82, 88.33, 90.1, 88.78, 70.81, 89.24, 89.45, 86.13, 75.5, 86.98, 89.72, 78.51, 89.14, 84.97, 85.2, 88.9, 86.81, 89.1, 88.74, 89.65, 72.63, 88.75, 88.93, 88.96, 83.78, 71.82, 85.62, 82.99, 89.0, 71.48, 72.65, 84.94, 78.16, 88.09, 88.85, 88.71, 88.39, 88.91, 33.86, 66.5, 88.01, 86.21, 85.95, 88.72, 86.01, 88.26, 72.35, 88.59, 85.86, 88.68, 88.94, 71.43, 84.01, 90.33, 89.32, 78.58, 79.69, 88.91, 87.98, 88.95, 88.98, 89.93, 90.56, 88.04, 88.18, 89.18, 84.03, 90.95, 79.55, 89.51]\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mloading model from file runs/RandomSearch/NasBench201SearchSpace/cifar10/333/search/model_final.pth\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (3, 2, 0, 0, 3, 2)\n",
            "\u001b[32m[10/08 11:22:14 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 90.95\n",
            "\u001b[32m[10/08 11:22:15 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs/RandomSearch/NasBench201SearchSpace/cifar10/444\n",
            "search:\n",
            "  checkpoint_freq: 200\n",
            "  epochs: 100\n",
            "  fidelity: -1\n",
            "  population_size: 30\n",
            "  sample_size: 10\n",
            "  seed: 444\n",
            "\u001b[32m[10/08 11:22:15 naslib]: \u001b[0mLoading Benchmark API\n",
            "\u001b[32m[10/08 11:22:16 nl.defaults.trainer]: \u001b[0mBeginning search\n",
            "\u001b[32m[10/08 11:22:16 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 96.48400, Validation accuracy: 84.99000\n",
            "\u001b[32m[10/08 11:22:16 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 99.98400, Validation accuracy: 89.59000\n",
            "\u001b[32m[10/08 11:22:17 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 99.96400, Validation accuracy: 89.77000\n",
            "\u001b[32m[10/08 11:22:17 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 99.97200, Validation accuracy: 89.29000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 99.80400, Validation accuracy: 86.00000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy: 99.76800, Validation accuracy: 88.02000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy: 99.96000, Validation accuracy: 90.07000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy: 99.92000, Validation accuracy: 89.08000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy: 100.00000, Validation accuracy: 91.15000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy: 79.54000, Validation accuracy: 74.85000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy: 96.30000, Validation accuracy: 85.59000\n",
            "\u001b[32m[10/08 11:22:18 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy: 97.95200, Validation accuracy: 85.98000\n",
            "\u001b[32m[10/08 11:22:19 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy: 99.93600, Validation accuracy: 89.65000\n",
            "\u001b[32m[10/08 11:22:19 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy: 99.97200, Validation accuracy: 90.32000\n",
            "\u001b[32m[10/08 11:22:20 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy: 97.62400, Validation accuracy: 85.98000\n",
            "\u001b[32m[10/08 11:22:20 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy: 99.93600, Validation accuracy: 89.16000\n",
            "\u001b[32m[10/08 11:22:20 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy: 99.88400, Validation accuracy: 89.64000\n",
            "\u001b[32m[10/08 11:22:20 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy: 79.52400, Validation accuracy: 74.86000\n",
            "\u001b[32m[10/08 11:22:20 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy: 99.98400, Validation accuracy: 90.45000\n",
            "\u001b[32m[10/08 11:22:20 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy: 99.24400, Validation accuracy: 84.74000\n",
            "\u001b[32m[10/08 11:22:20 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy: 99.97200, Validation accuracy: 90.22000\n",
            "\u001b[32m[10/08 11:22:21 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy: 96.60400, Validation accuracy: 85.90000\n",
            "\u001b[32m[10/08 11:22:21 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy: 99.48800, Validation accuracy: 87.33000\n",
            "\u001b[32m[10/08 11:22:22 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy: 80.86000, Validation accuracy: 71.42000\n",
            "\u001b[32m[10/08 11:22:22 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy: 99.91200, Validation accuracy: 88.61000\n",
            "\u001b[32m[10/08 11:22:22 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy: 99.68400, Validation accuracy: 86.31000\n",
            "\u001b[32m[10/08 11:22:22 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy: 99.92800, Validation accuracy: 88.26000\n",
            "\u001b[32m[10/08 11:22:22 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy: 81.22400, Validation accuracy: 77.46000\n",
            "\u001b[32m[10/08 11:22:22 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy: 98.56000, Validation accuracy: 86.44000\n",
            "\u001b[32m[10/08 11:22:22 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy: 81.10000, Validation accuracy: 71.41000\n",
            "\u001b[32m[10/08 11:22:23 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy: 82.50400, Validation accuracy: 77.88000\n",
            "\u001b[32m[10/08 11:22:23 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy: 99.98400, Validation accuracy: 89.97000\n",
            "\u001b[32m[10/08 11:22:23 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy: 99.95600, Validation accuracy: 88.36000\n",
            "\u001b[32m[10/08 11:22:23 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy: 99.62800, Validation accuracy: 86.50000\n",
            "\u001b[32m[10/08 11:22:24 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy: 91.44800, Validation accuracy: 82.42000\n",
            "\u001b[32m[10/08 11:22:24 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy: 71.94000, Validation accuracy: 66.15000\n",
            "\u001b[32m[10/08 11:22:24 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy: 81.19200, Validation accuracy: 71.82000\n",
            "\u001b[32m[10/08 11:22:24 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy: 99.82800, Validation accuracy: 87.97000\n",
            "\u001b[32m[10/08 11:22:25 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy: 99.82000, Validation accuracy: 85.67000\n",
            "\u001b[32m[10/08 11:22:25 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy: 99.89600, Validation accuracy: 86.79000\n",
            "\u001b[32m[10/08 11:22:25 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy: 99.96400, Validation accuracy: 89.26000\n",
            "\u001b[32m[10/08 11:22:25 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy: 91.65200, Validation accuracy: 83.89000\n",
            "\u001b[32m[10/08 11:22:25 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy: 99.92800, Validation accuracy: 88.43000\n",
            "\u001b[32m[10/08 11:22:25 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy: 80.97200, Validation accuracy: 71.87000\n",
            "\u001b[32m[10/08 11:22:25 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy: 99.46400, Validation accuracy: 86.21000\n",
            "\u001b[32m[10/08 11:22:26 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy: 99.92800, Validation accuracy: 88.60000\n",
            "\u001b[32m[10/08 11:22:26 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy: 99.56000, Validation accuracy: 87.40000\n",
            "\u001b[32m[10/08 11:22:26 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy: 99.91200, Validation accuracy: 87.21000\n",
            "\u001b[32m[10/08 11:22:26 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy: 99.98000, Validation accuracy: 90.99000\n",
            "\u001b[32m[10/08 11:22:27 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy: 99.91600, Validation accuracy: 88.84000\n",
            "\u001b[32m[10/08 11:22:27 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy: 99.84000, Validation accuracy: 88.38000\n",
            "\u001b[32m[10/08 11:22:27 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy: 90.82800, Validation accuracy: 83.78000\n",
            "\u001b[32m[10/08 11:22:28 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy: 99.84400, Validation accuracy: 86.73000\n",
            "\u001b[32m[10/08 11:22:28 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy: 99.90800, Validation accuracy: 88.97000\n",
            "\u001b[32m[10/08 11:22:28 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy: 99.93600, Validation accuracy: 89.04000\n",
            "\u001b[32m[10/08 11:22:28 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy: 89.90400, Validation accuracy: 71.76000\n",
            "\u001b[32m[10/08 11:22:28 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy: 99.60000, Validation accuracy: 87.68000\n",
            "\u001b[32m[10/08 11:22:28 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy: 99.27200, Validation accuracy: 83.42000\n",
            "\u001b[32m[10/08 11:22:28 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy: 90.77600, Validation accuracy: 81.19000\n",
            "\u001b[32m[10/08 11:22:29 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy: 99.96800, Validation accuracy: 89.53000\n",
            "\u001b[32m[10/08 11:22:29 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy: 91.92000, Validation accuracy: 83.65000\n",
            "\u001b[32m[10/08 11:22:29 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy: 98.85600, Validation accuracy: 86.15000\n",
            "\u001b[32m[10/08 11:22:29 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy: 93.71200, Validation accuracy: 84.33000\n",
            "\u001b[32m[10/08 11:22:29 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy: 99.93600, Validation accuracy: 88.08000\n",
            "\u001b[32m[10/08 11:22:29 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy: 90.88400, Validation accuracy: 83.71000\n",
            "\u001b[32m[10/08 11:22:29 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy: 95.82800, Validation accuracy: 83.78000\n",
            "\u001b[32m[10/08 11:22:30 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy: 33.03600, Validation accuracy: 33.92000\n",
            "\u001b[32m[10/08 11:22:31 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy: 99.89200, Validation accuracy: 88.53000\n",
            "\u001b[32m[10/08 11:22:31 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy: 94.54800, Validation accuracy: 84.56000\n",
            "\u001b[32m[10/08 11:22:31 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy: 99.91600, Validation accuracy: 88.76000\n",
            "\u001b[32m[10/08 11:22:31 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy: 91.07200, Validation accuracy: 82.95000\n",
            "\u001b[32m[10/08 11:22:31 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy: 99.95600, Validation accuracy: 90.11000\n",
            "\u001b[32m[10/08 11:22:32 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy: 98.68000, Validation accuracy: 86.54000\n",
            "\u001b[32m[10/08 11:22:32 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy: 99.91200, Validation accuracy: 89.09000\n",
            "\u001b[32m[10/08 11:22:32 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy: 99.96000, Validation accuracy: 89.08000\n",
            "\u001b[32m[10/08 11:22:32 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy: 99.86400, Validation accuracy: 88.78000\n",
            "\u001b[32m[10/08 11:22:32 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy: 99.27200, Validation accuracy: 85.33000\n",
            "\u001b[32m[10/08 11:22:32 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy: 99.91600, Validation accuracy: 88.67000\n",
            "\u001b[32m[10/08 11:22:32 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy: 99.96400, Validation accuracy: 88.87000\n",
            "\u001b[32m[10/08 11:22:33 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy: 99.95200, Validation accuracy: 89.62000\n",
            "\u001b[32m[10/08 11:22:33 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy: 99.65200, Validation accuracy: 84.09000\n",
            "\u001b[32m[10/08 11:22:33 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy: 99.98400, Validation accuracy: 90.83000\n",
            "\u001b[32m[10/08 11:22:33 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy: 99.93200, Validation accuracy: 89.25000\n",
            "\u001b[32m[10/08 11:22:33 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy: 99.72000, Validation accuracy: 87.88000\n",
            "\u001b[32m[10/08 11:22:33 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy: 99.86400, Validation accuracy: 88.18000\n",
            "\u001b[32m[10/08 11:22:33 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy: 99.92800, Validation accuracy: 87.67000\n",
            "\u001b[32m[10/08 11:22:34 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy: 99.14800, Validation accuracy: 86.85000\n",
            "\u001b[32m[10/08 11:22:34 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy: 77.83600, Validation accuracy: 74.21000\n",
            "\u001b[32m[10/08 11:22:34 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy: 90.95600, Validation accuracy: 84.16000\n",
            "\u001b[32m[10/08 11:22:35 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy: 99.25200, Validation accuracy: 85.30000\n",
            "\u001b[32m[10/08 11:22:35 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy: 99.98400, Validation accuracy: 89.78000\n",
            "\u001b[32m[10/08 11:22:36 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy: 99.31200, Validation accuracy: 83.35000\n",
            "\u001b[32m[10/08 11:22:36 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy: 96.46800, Validation accuracy: 85.50000\n",
            "\u001b[32m[10/08 11:22:36 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy: 99.86800, Validation accuracy: 88.34000\n",
            "\u001b[32m[10/08 11:22:36 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy: 71.98800, Validation accuracy: 69.32000\n",
            "\u001b[32m[10/08 11:22:36 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy: 96.48800, Validation accuracy: 85.80000\n",
            "\u001b[32m[10/08 11:22:36 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy: 90.90400, Validation accuracy: 75.80000\n",
            "\u001b[32m[10/08 11:22:36 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy: 99.92000, Validation accuracy: 89.50000\n",
            "\u001b[32m[10/08 11:22:37 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy: 98.88000, Validation accuracy: 86.40000\n",
            "\u001b[32m[10/08 11:22:37 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy: 99.98400, Validation accuracy: 89.97000\n",
            "\u001b[32m[10/08 11:22:37 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [96.48400001708984, 99.984, 99.964, 99.972, 99.80400000244141, 99.768, 99.96, 99.92, 100.0, 79.53999997558594, 96.30000001708984, 97.95200000732422, 99.936, 99.972, 97.62399997070312, 99.936, 99.88400000488281, 79.52399999023437, 99.984, 99.24400000976563, 99.972, 96.60399998046876, 99.4880000048828, 80.85999997070313, 99.912, 99.684, 99.92800000244141, 81.22399997314453, 98.56000000732422, 81.09999998535156, 82.50399997070312, 99.984, 99.956, 99.62800000244141, 91.44799999267578, 71.94000001953125, 81.19199998291016, 99.8280000024414, 99.8200000024414, 99.896, 99.964, 91.65199998046874, 99.928, 80.97199997070312, 99.46400000244141, 99.928, 99.56, 99.912, 99.98, 99.91600000244141, 99.84, 90.82800001464844, 99.844, 99.908, 99.936, 89.90399999511719, 99.6, 99.272, 90.7760000024414, 99.968, 91.91999999267578, 98.85600000732421, 93.71199997314453, 99.936, 90.88399998291015, 95.82800001464844, 33.036000004882816, 99.892, 94.54799999023437, 99.916, 91.07199998779296, 99.956, 98.68000000976562, 99.912, 99.96, 99.86400000488281, 99.272, 99.91600000244141, 99.964, 99.952, 99.652, 99.984, 99.932, 99.72000000244141, 99.864, 99.928, 99.14800000488282, 77.836, 90.956, 99.25200000732421, 99.984, 99.31200000244141, 96.46800001708985, 99.868, 71.98799999267578, 96.48799997314453, 90.90399997802734, 99.92000000244141, 98.88000000488282, 99.984]\n",
            "Validation accuracies: [84.99, 89.59, 89.77, 89.29, 86.0, 88.02, 90.07, 89.08, 91.15, 74.85, 85.59, 85.98, 89.65, 90.32, 85.98, 89.16, 89.64, 74.86, 90.45, 84.74, 90.22, 85.9, 87.33, 71.42, 88.61, 86.31, 88.26, 77.46, 86.44, 71.41, 77.88, 89.97, 88.36, 86.5, 82.42, 66.15, 71.82, 87.97, 85.67, 86.79, 89.26, 83.89, 88.43, 71.87, 86.21, 88.6, 87.4, 87.21, 90.99, 88.84, 88.38, 83.78, 86.73, 88.97, 89.04, 71.76, 87.68, 83.42, 81.19, 89.53, 83.65, 86.15, 84.33, 88.08, 83.71, 83.78, 33.92, 88.53, 84.56, 88.76, 82.95, 90.11, 86.54, 89.09, 89.08, 88.78, 85.33, 88.67, 88.87, 89.62, 84.09, 90.83, 89.25, 87.88, 88.18, 87.67, 86.85, 74.21, 84.16, 85.3, 89.78, 83.35, 85.5, 88.34, 69.32, 85.8, 75.8, 89.5, 86.4, 89.97]\n",
            "\u001b[32m[10/08 11:22:37 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/08 11:22:37 nl.defaults.trainer]: \u001b[0mloading model from file runs/RandomSearch/NasBench201SearchSpace/cifar10/444/search/model_final.pth\n",
            "\u001b[32m[10/08 11:22:37 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (2, 2, 0, 0, 3, 2)\n",
            "\u001b[32m[10/08 11:22:37 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 91.15\n",
            "\u001b[32m[10/08 11:22:37 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs/RandomSearch/NasBench201SearchSpace/cifar10/555\n",
            "search:\n",
            "  checkpoint_freq: 200\n",
            "  epochs: 100\n",
            "  fidelity: -1\n",
            "  population_size: 30\n",
            "  sample_size: 10\n",
            "  seed: 555\n",
            "\u001b[32m[10/08 11:22:37 naslib]: \u001b[0mLoading Benchmark API\n",
            "\u001b[32m[10/08 11:22:38 nl.defaults.trainer]: \u001b[0mBeginning search\n",
            "\u001b[32m[10/08 11:22:39 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 97.52400, Validation accuracy: 84.15000\n",
            "\u001b[32m[10/08 11:22:39 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 99.90400, Validation accuracy: 87.58000\n",
            "\u001b[32m[10/08 11:22:39 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 99.90800, Validation accuracy: 88.19000\n",
            "\u001b[32m[10/08 11:22:39 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 99.24800, Validation accuracy: 86.13000\n",
            "\u001b[32m[10/08 11:22:39 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 98.46400, Validation accuracy: 84.65000\n",
            "\u001b[32m[10/08 11:22:39 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy: 91.12800, Validation accuracy: 71.99000\n",
            "\u001b[32m[10/08 11:22:39 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy: 89.96400, Validation accuracy: 82.45000\n",
            "\u001b[32m[10/08 11:22:40 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy: 99.90800, Validation accuracy: 86.87000\n",
            "\u001b[32m[10/08 11:22:40 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy: 97.37600, Validation accuracy: 79.70000\n",
            "\u001b[32m[10/08 11:22:41 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy: 91.12800, Validation accuracy: 83.75000\n",
            "\u001b[32m[10/08 11:22:41 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy: 99.98000, Validation accuracy: 90.53000\n",
            "\u001b[32m[10/08 11:22:41 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy: 81.50400, Validation accuracy: 77.89000\n",
            "\u001b[32m[10/08 11:22:41 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy: 99.92400, Validation accuracy: 88.89000\n",
            "\u001b[32m[10/08 11:22:41 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy: 90.52400, Validation accuracy: 83.84000\n",
            "\u001b[32m[10/08 11:22:41 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy: 99.40000, Validation accuracy: 87.03000\n",
            "\u001b[32m[10/08 11:22:41 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy: 99.06400, Validation accuracy: 85.51000\n",
            "\u001b[32m[10/08 11:22:42 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy: 99.72800, Validation accuracy: 86.07000\n",
            "\u001b[32m[10/08 11:22:42 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy: 99.18000, Validation accuracy: 86.90000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy: 70.35600, Validation accuracy: 67.97000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy: 90.78800, Validation accuracy: 84.23000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy: 88.54000, Validation accuracy: 80.24000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy: 99.90800, Validation accuracy: 87.73000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy: 99.98000, Validation accuracy: 88.69000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy: 99.92800, Validation accuracy: 88.07000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy: 99.32400, Validation accuracy: 85.43000\n",
            "\u001b[32m[10/08 11:22:43 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy: 94.47600, Validation accuracy: 76.43000\n",
            "\u001b[32m[10/08 11:22:44 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy: 90.19600, Validation accuracy: 82.22000\n",
            "\u001b[32m[10/08 11:22:44 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy: 99.96800, Validation accuracy: 88.69000\n",
            "\u001b[32m[10/08 11:22:45 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy: 99.28800, Validation accuracy: 85.08000\n",
            "\u001b[32m[10/08 11:22:45 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy: 99.90400, Validation accuracy: 88.94000\n",
            "\u001b[32m[10/08 11:22:45 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy: 93.16000, Validation accuracy: 83.99000\n",
            "\u001b[32m[10/08 11:22:45 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy: 99.97200, Validation accuracy: 89.89000\n",
            "\u001b[32m[10/08 11:22:45 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy: 99.75200, Validation accuracy: 87.79000\n",
            "\u001b[32m[10/08 11:22:45 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy: 99.98800, Validation accuracy: 88.95000\n",
            "\u001b[32m[10/08 11:22:46 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy: 99.99200, Validation accuracy: 90.01000\n",
            "\u001b[32m[10/08 11:22:46 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy: 99.95600, Validation accuracy: 89.21000\n",
            "\u001b[32m[10/08 11:22:46 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy: 99.94000, Validation accuracy: 87.24000\n",
            "\u001b[32m[10/08 11:22:46 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy: 99.58000, Validation accuracy: 86.48000\n",
            "\u001b[32m[10/08 11:22:46 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy: 99.93600, Validation accuracy: 89.09000\n",
            "\u001b[32m[10/08 11:22:46 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy: 99.93600, Validation accuracy: 88.08000\n",
            "\u001b[32m[10/08 11:22:46 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy: 98.40800, Validation accuracy: 87.00000\n",
            "\u001b[32m[10/08 11:22:47 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy: 99.76400, Validation accuracy: 87.65000\n",
            "\u001b[32m[10/08 11:22:48 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy: 99.89600, Validation accuracy: 87.51000\n",
            "\u001b[32m[10/08 11:22:48 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy: 96.25600, Validation accuracy: 85.72000\n",
            "\u001b[32m[10/08 11:22:48 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy: 99.93600, Validation accuracy: 88.58000\n",
            "\u001b[32m[10/08 11:22:48 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy: 99.95600, Validation accuracy: 88.71000\n",
            "\u001b[32m[10/08 11:22:48 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy: 91.44800, Validation accuracy: 82.42000\n",
            "\u001b[32m[10/08 11:22:48 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy: 86.15600, Validation accuracy: 81.96000\n",
            "\u001b[32m[10/08 11:22:48 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy: 99.45200, Validation accuracy: 85.13000\n",
            "\u001b[32m[10/08 11:22:49 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy: 99.91600, Validation accuracy: 89.73000\n",
            "\u001b[32m[10/08 11:22:49 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy: 99.99200, Validation accuracy: 90.96000\n",
            "\u001b[32m[10/08 11:22:49 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy: 81.30400, Validation accuracy: 71.97000\n",
            "\u001b[32m[10/08 11:22:49 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy: 99.58000, Validation accuracy: 86.83000\n",
            "\u001b[32m[10/08 11:22:49 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy: 99.92400, Validation accuracy: 87.26000\n",
            "\u001b[32m[10/08 11:22:49 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy: 97.26000, Validation accuracy: 85.61000\n",
            "\u001b[32m[10/08 11:22:50 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy: 80.66000, Validation accuracy: 77.02000\n",
            "\u001b[32m[10/08 11:22:50 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy: 100.00000, Validation accuracy: 90.82000\n",
            "\u001b[32m[10/08 11:22:51 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy: 99.98000, Validation accuracy: 88.99000\n",
            "\u001b[32m[10/08 11:22:51 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy: 99.95200, Validation accuracy: 89.49000\n",
            "\u001b[32m[10/08 11:22:51 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy: 71.14000, Validation accuracy: 68.50000\n",
            "\u001b[32m[10/08 11:22:51 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy: 99.51200, Validation accuracy: 87.69000\n",
            "\u001b[32m[10/08 11:22:51 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy: 90.88000, Validation accuracy: 83.64000\n",
            "\u001b[32m[10/08 11:22:52 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy: 99.96000, Validation accuracy: 89.36000\n",
            "\u001b[32m[10/08 11:22:52 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy: 99.64400, Validation accuracy: 87.46000\n",
            "\u001b[32m[10/08 11:22:52 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy: 92.02400, Validation accuracy: 82.41000\n",
            "\u001b[32m[10/08 11:22:52 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy: 60.70800, Validation accuracy: 60.01000\n",
            "\u001b[32m[10/08 11:22:52 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy: 93.99200, Validation accuracy: 85.14000\n",
            "\u001b[32m[10/08 11:22:52 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy: 99.64800, Validation accuracy: 86.59000\n",
            "\u001b[32m[10/08 11:22:52 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy: 95.46400, Validation accuracy: 82.66000\n",
            "\u001b[32m[10/08 11:22:53 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy: 99.93600, Validation accuracy: 89.14000\n",
            "\u001b[32m[10/08 11:22:53 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy: 99.91600, Validation accuracy: 88.33000\n",
            "\u001b[32m[10/08 11:22:53 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy: 99.91200, Validation accuracy: 88.71000\n",
            "\u001b[32m[10/08 11:22:53 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy: 99.82800, Validation accuracy: 88.34000\n",
            "\u001b[32m[10/08 11:22:53 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy: 99.93200, Validation accuracy: 88.64000\n",
            "\u001b[32m[10/08 11:22:53 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy: 99.94000, Validation accuracy: 87.34000\n",
            "\u001b[32m[10/08 11:22:53 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy: 99.21200, Validation accuracy: 86.35000\n",
            "\u001b[32m[10/08 11:22:54 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy: 99.26400, Validation accuracy: 86.77000\n",
            "\u001b[32m[10/08 11:22:55 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy: 99.95600, Validation accuracy: 88.28000\n",
            "\u001b[32m[10/08 11:22:55 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy: 99.98000, Validation accuracy: 88.71000\n",
            "\u001b[32m[10/08 11:22:55 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy: 96.63200, Validation accuracy: 86.02000\n",
            "\u001b[32m[10/08 11:22:55 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy: 97.18400, Validation accuracy: 85.36000\n",
            "\u001b[32m[10/08 11:22:55 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy: 99.63200, Validation accuracy: 87.87000\n",
            "\u001b[32m[10/08 11:22:56 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy: 98.81200, Validation accuracy: 85.59000\n",
            "\u001b[32m[10/08 11:22:56 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy: 99.92800, Validation accuracy: 87.39000\n",
            "\u001b[32m[10/08 11:22:56 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy: 80.11600, Validation accuracy: 76.33000\n",
            "\u001b[32m[10/08 11:22:56 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy: 99.92000, Validation accuracy: 88.42000\n",
            "\u001b[32m[10/08 11:22:56 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy: 96.39200, Validation accuracy: 86.00000\n",
            "\u001b[32m[10/08 11:22:56 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy: 90.55600, Validation accuracy: 83.58000\n",
            "\u001b[32m[10/08 11:22:56 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy: 99.34000, Validation accuracy: 86.66000\n",
            "\u001b[32m[10/08 11:22:57 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy: 97.58400, Validation accuracy: 79.64000\n",
            "\u001b[32m[10/08 11:22:57 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy: 99.84800, Validation accuracy: 86.82000\n",
            "\u001b[32m[10/08 11:22:57 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy: 99.98800, Validation accuracy: 89.50000\n",
            "\u001b[32m[10/08 11:22:57 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy: 80.92800, Validation accuracy: 70.90000\n",
            "\u001b[32m[10/08 11:22:57 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy: 81.62800, Validation accuracy: 71.24000\n",
            "\u001b[32m[10/08 11:22:57 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy: 76.02000, Validation accuracy: 72.17000\n",
            "\u001b[32m[10/08 11:22:57 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy: 99.94400, Validation accuracy: 89.01000\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy: 99.93200, Validation accuracy: 88.28000\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy: 97.95200, Validation accuracy: 85.98000\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy: 80.34800, Validation accuracy: 70.72000\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy: 99.62000, Validation accuracy: 87.25000\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [97.52400001464844, 99.904, 99.908, 99.2480000024414, 98.4640000048828, 91.12800000488281, 89.96399998535156, 99.908, 97.37600000976562, 91.12799998535156, 99.98, 81.50399998046875, 99.9240000024414, 90.5239999951172, 99.40000000488281, 99.0640000024414, 99.728, 99.18, 70.35599998779297, 90.78799998779297, 88.53999999511718, 99.90800000244141, 99.98, 99.928, 99.32400000732422, 94.47599997558594, 90.19599999755859, 99.968, 99.28800000732421, 99.904, 93.15999998535156, 99.972, 99.752, 99.988, 99.992, 99.956, 99.94, 99.58000000488282, 99.936, 99.936, 98.40800001220703, 99.764, 99.896, 96.25600001953126, 99.936, 99.956, 91.44799999267578, 86.15600000732422, 99.45200000244141, 99.916, 99.992, 81.30399997558594, 99.58, 99.924, 97.26000001464844, 80.65999997314454, 100.0, 99.98, 99.952, 71.13999997802735, 99.51200000732422, 90.87999998779297, 99.96, 99.6440000024414, 92.02399998779296, 60.70799998413086, 93.99199997314453, 99.6480000024414, 95.46400001464843, 99.936, 99.916, 99.912, 99.8280000024414, 99.932, 99.94, 99.212, 99.26400000488282, 99.956, 99.98, 96.63200001464844, 97.18399997070313, 99.632, 98.81200000244141, 99.928, 80.11599997070313, 99.92, 96.39200001220703, 90.55599998779297, 99.3400000024414, 97.58400000732422, 99.848, 99.988, 80.92799998046875, 81.62799998291015, 76.02000000488282, 99.944, 99.932, 97.95200000732422, 80.34799998291015, 99.62]\n",
            "Validation accuracies: [84.15, 87.58, 88.19, 86.13, 84.65, 71.99, 82.45, 86.87, 79.7, 83.75, 90.53, 77.89, 88.89, 83.84, 87.03, 85.51, 86.07, 86.9, 67.97, 84.23, 80.24, 87.73, 88.69, 88.07, 85.43, 76.43, 82.22, 88.69, 85.08, 88.94, 83.99, 89.89, 87.79, 88.95, 90.01, 89.21, 87.24, 86.48, 89.09, 88.08, 87.0, 87.65, 87.51, 85.72, 88.58, 88.71, 82.42, 81.96, 85.13, 89.73, 90.96, 71.97, 86.83, 87.26, 85.61, 77.02, 90.82, 88.99, 89.49, 68.5, 87.69, 83.64, 89.36, 87.46, 82.41, 60.01, 85.14, 86.59, 82.66, 89.14, 88.33, 88.71, 88.34, 88.64, 87.34, 86.35, 86.77, 88.28, 88.71, 86.02, 85.36, 87.87, 85.59, 87.39, 76.33, 88.42, 86.0, 83.58, 86.66, 79.64, 86.82, 89.5, 70.9, 71.24, 72.17, 89.01, 88.28, 85.98, 70.72, 87.25]\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mloading model from file runs/RandomSearch/NasBench201SearchSpace/cifar10/555/search/model_final.pth\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (1, 2, 0, 0, 4, 2)\n",
            "\u001b[32m[10/08 11:22:58 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 90.96\n"
          ]
        }
      ],
      "source": [
        "# Set the optimizer and search space types\n",
        "# They will be instantiated inside run_optimizer\n",
        "optimizer_type = RandomSearch # {RegularizedEvolution, RandomSearch}\n",
        "search_space_type = NasBench201SearchSpace # {NasBench101SearchSpace, NasBench201SearchSpace, NasBench301SearchSpace}\n",
        "\n",
        "# Set the dataset\n",
        "dataset = 'cifar10' # cifar10 for NB101 and NB301, {cifar100, ImageNet16-120} for NB201\n",
        "\n",
        "# The configuration used by the Trainer and Optimizer\n",
        "# The missing information will be populated inside run_optimizer\n",
        "config = {\n",
        "    'search': {\n",
        "        # Required by Trainer\n",
        "        'epochs': 100,\n",
        "        'checkpoint_freq': 200,\n",
        "        \n",
        "        # Required by Random Search optimizer\n",
        "        'fidelity': -1,\n",
        "        \n",
        "        # Required by RegularizedEvolution\n",
        "        'sample_size': 10,\n",
        "        'population_size': 30,\n",
        "    }\n",
        "}\n",
        "config = CfgNode.load_cfg(json.dumps(config))\n",
        "#Initialize trajectories for the 3 seeds\n",
        "trajectories_rs[\"RS\"] = []\n",
        "for seed in [333,444,555]: #Run RS for 3 seeds\n",
        "  search_trajectory, best_model, best_model_val_acc = run_optimizer(\n",
        "                                                        optimizer_type,\n",
        "                                                        search_space_type,\n",
        "                                                        dataset,\n",
        "                                                        config,\n",
        "                                                        seed\n",
        "                                                    )\n",
        "  trajectories_rs[\"RS\"].append(search_trajectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "RX-p_kyfBgLP",
        "outputId": "96779ea1-cc60-4e1a-f056-f9e3579c9143"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot RS for three seeds\n",
        "plot_optimizers(trajectories_rs,\"NAS-Bench-201 cifar10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gvlzRSbTHQ-"
      },
      "source": [
        "### TASK 2: Search using the Regularized Evolution Optimizer\n",
        "1. Search using the Regularized Evolution Optimizer for 100 epochs\n",
        "2. Use NASBench301 search space with cifar10 dataset\n",
        "2. Plot the search trajectories of RE for cifar10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH2nBnIhV2xt"
      },
      "source": [
        "## Regularized Evolution\n",
        "![](https://drive.google.com/uc?id=11rF0ZQSoth4dMe8G5tmdemw2WBmXBerF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8d2aQBShTwhu"
      },
      "outputs": [],
      "source": [
        "trajectories_re= {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FPUcWjSUBV8"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# 1. Use the Regularized Evolution optimizer instead of RandomSearch \n",
        "# 2. Use the NASBench301SearchSpace for cifar100 \n",
        "# 3. Plot the RE search trajectory for NASBench301 and cifar10\n",
        "\n",
        "# Set the optimizer type\n",
        "optimizer_type = ...\n",
        "# Set the search space type\n",
        "search_space_type = ...\n",
        "\n",
        "# Set the dataset\n",
        "dataset = ... \n",
        "\n",
        "# The configuration used by the Trainer and Optimizer\n",
        "# The missing information will be populated inside run_optimizer\n",
        "config = {\n",
        "    'search': {\n",
        "        # Required by Trainer\n",
        "        'epochs': 100,\n",
        "        'checkpoint_freq': 100,\n",
        "        \n",
        "        # Required by Random Search optimizer\n",
        "        'fidelity': -1,\n",
        "        \n",
        "        # Required by RegularizedEvolution\n",
        "        'sample_size': 10,\n",
        "        'population_size': 30,\n",
        "    }\n",
        "}\n",
        "config = CfgNode.load_cfg(json.dumps(config))\n",
        "#Initialize trajectories for the 3 seeds\n",
        "trajectories_re[\"RE\"] = []\n",
        "for seed in [333,444,555]: #Run RS for 3 seeds\n",
        "  search_trajectory, best_model, best_model_val_acc = run_optimizer(\n",
        "                                                        optimizer_type,\n",
        "                                                        search_space_type,\n",
        "                                                        dataset,\n",
        "                                                        config,\n",
        "                                                        seed\n",
        "                                                    )\n",
        "  trajectories_re[\"RE\"].append(search_trajectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9az6ZCzKDXBZ"
      },
      "outputs": [],
      "source": [
        "# Plot RS and RE for three seeds\n",
        "plot_optimizers(trajectories_re,\"NAS-Bench-301 cifar10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmG74Qu8W0YT"
      },
      "source": [
        "### BANANAS\n",
        "![](https://drive.google.com/uc?id=1KelEs9KUXpyO1MiO_jJ4b7gQSMWgJBlz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "tdIZXp5hX-HZ"
      },
      "outputs": [],
      "source": [
        "#initialize the trajectory for BANANAS\n",
        "trajectories_bn={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kyDMcY1ColRu",
        "outputId": "72f8377b-8fa6-4b28-defd-2f156f8b5fac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/08 11:51:20 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs/Bananas/NasBench201SearchSpace/cifar10/333\n",
            "search:\n",
            "  acq_fn_optimization: random_sampling\n",
            "  acq_fn_type: its\n",
            "  checkpoint_freq: 100\n",
            "  encoding_type: path\n",
            "  epochs: 100\n",
            "  fidelity: -1\n",
            "  k: 20\n",
            "  max_mutations: 1\n",
            "  num_arches_to_mutate: 5\n",
            "  num_candidates: 100\n",
            "  num_ensemble: 1\n",
            "  num_init: 10\n",
            "  population_size: 30\n",
            "  predictor_type: xgb\n",
            "  sample_size: 10\n",
            "  seed: 333\n",
            "\u001b[32m[10/08 11:51:20 naslib]: \u001b[0mLoading Benchmark API\n",
            "\u001b[32m[10/08 11:51:21 nl.defaults.trainer]: \u001b[0mBeginning search\n",
            "\u001b[32m[10/08 11:51:21 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 99.54800, Validation accuracy: 83.83000\n",
            "\u001b[32m[10/08 11:51:21 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 72.61200, Validation accuracy: 66.22000\n",
            "\u001b[32m[10/08 11:51:21 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 99.91200, Validation accuracy: 88.09000\n",
            "\u001b[32m[10/08 11:51:22 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 99.88400, Validation accuracy: 88.68000\n",
            "\u001b[32m[10/08 11:51:22 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 99.92000, Validation accuracy: 89.14000\n",
            "\u001b[32m[10/08 11:51:22 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy: 99.98800, Validation accuracy: 90.01000\n",
            "\u001b[32m[10/08 11:51:22 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy: 97.94400, Validation accuracy: 85.91000\n",
            "\u001b[32m[10/08 11:51:23 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy: 89.03200, Validation accuracy: 81.87000\n",
            "\u001b[32m[10/08 11:51:23 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy: 72.39200, Validation accuracy: 66.30000\n",
            "\u001b[32m[10/08 11:51:23 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy: 99.52800, Validation accuracy: 87.35000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy: 99.97200, Validation accuracy: 90.10000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy: 99.98800, Validation accuracy: 90.56000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy: 99.98400, Validation accuracy: 90.95000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy: 99.95200, Validation accuracy: 88.68000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy: 99.94400, Validation accuracy: 89.93000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy: 99.92000, Validation accuracy: 89.51000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy: 99.96400, Validation accuracy: 88.33000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy: 99.88800, Validation accuracy: 88.93000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy: 99.90400, Validation accuracy: 86.58000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy: 90.96800, Validation accuracy: 84.01000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy: 99.95600, Validation accuracy: 88.94000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy: 91.50400, Validation accuracy: 82.99000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy: 99.96400, Validation accuracy: 88.91000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy: 99.75200, Validation accuracy: 87.90000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy: 98.34400, Validation accuracy: 84.97000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy: 75.50000, Validation accuracy: 72.65000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy: 99.98800, Validation accuracy: 90.33000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy: 99.95600, Validation accuracy: 90.20000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy: 91.22000, Validation accuracy: 84.73000\n",
            "\u001b[32m[10/08 11:51:45 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy: 98.07200, Validation accuracy: 84.94000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy: 99.98000, Validation accuracy: 90.38000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy: 99.99600, Validation accuracy: 90.66000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy: 99.60400, Validation accuracy: 87.60000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy: 99.94000, Validation accuracy: 88.77000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy: 99.93200, Validation accuracy: 88.94000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy: 98.83200, Validation accuracy: 86.67000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy: 99.30000, Validation accuracy: 87.15000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy: 98.46000, Validation accuracy: 85.48000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy: 96.00000, Validation accuracy: 85.33000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy: 99.96400, Validation accuracy: 89.21000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy: 99.92400, Validation accuracy: 88.85000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy: 99.39200, Validation accuracy: 87.00000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy: 99.97600, Validation accuracy: 90.23000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy: 99.90800, Validation accuracy: 87.80000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy: 99.91600, Validation accuracy: 88.55000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy: 99.84400, Validation accuracy: 88.28000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy: 99.92000, Validation accuracy: 88.67000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy: 99.83200, Validation accuracy: 88.10000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy: 99.98400, Validation accuracy: 89.48000\n",
            "\u001b[32m[10/08 11:52:07 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy: 99.94800, Validation accuracy: 89.49000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy: 99.19200, Validation accuracy: 86.46000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy: 99.98800, Validation accuracy: 90.56000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy: 99.84000, Validation accuracy: 88.29000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy: 99.95200, Validation accuracy: 89.68000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy: 99.98000, Validation accuracy: 88.75000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy: 99.49200, Validation accuracy: 86.63000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy: 99.92800, Validation accuracy: 89.22000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy: 99.92800, Validation accuracy: 88.08000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy: 98.33200, Validation accuracy: 86.62000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy: 99.97200, Validation accuracy: 89.00000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy: 99.98000, Validation accuracy: 88.65000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy: 99.94400, Validation accuracy: 88.57000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy: 99.93200, Validation accuracy: 89.30000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy: 99.96800, Validation accuracy: 89.64000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy: 99.97200, Validation accuracy: 88.00000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy: 99.94400, Validation accuracy: 88.83000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy: 99.92800, Validation accuracy: 88.35000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy: 99.91600, Validation accuracy: 88.72000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy: 99.74800, Validation accuracy: 87.86000\n",
            "\u001b[32m[10/08 11:52:29 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy: 99.96400, Validation accuracy: 88.05000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy: 99.97200, Validation accuracy: 89.37000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy: 99.97200, Validation accuracy: 89.43000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy: 99.98000, Validation accuracy: 89.08000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy: 99.96400, Validation accuracy: 88.18000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy: 99.98000, Validation accuracy: 89.06000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy: 99.94400, Validation accuracy: 89.45000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy: 99.89200, Validation accuracy: 87.31000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy: 99.96800, Validation accuracy: 89.30000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy: 99.91200, Validation accuracy: 88.83000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy: 99.96800, Validation accuracy: 89.55000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy: 99.95600, Validation accuracy: 89.33000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy: 99.98400, Validation accuracy: 89.46000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy: 99.96000, Validation accuracy: 89.00000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy: 99.93200, Validation accuracy: 88.71000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy: 99.97600, Validation accuracy: 89.62000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy: 99.96800, Validation accuracy: 89.50000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy: 99.98000, Validation accuracy: 88.65000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy: 99.91200, Validation accuracy: 89.61000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy: 99.86800, Validation accuracy: 87.77000\n",
            "\u001b[32m[10/08 11:52:50 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy: 99.52000, Validation accuracy: 87.23000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy: 99.98800, Validation accuracy: 90.33000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy: 86.70400, Validation accuracy: 82.09000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy: 99.96000, Validation accuracy: 89.56000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy: 99.95600, Validation accuracy: 89.43000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy: 99.91600, Validation accuracy: 88.45000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy: 99.94800, Validation accuracy: 89.05000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy: 99.99200, Validation accuracy: 90.24000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy: 78.73200, Validation accuracy: 73.95000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy: 99.98000, Validation accuracy: 88.85000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy: 99.76800, Validation accuracy: 88.02000\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [99.548, 72.61199998291016, 99.912, 99.88400000244141, 99.92, 99.988, 97.94400000732422, 89.03200001220704, 72.39200001953125, 99.528, 99.972, 99.988, 99.984, 99.952, 99.944, 99.92, 99.964, 99.888, 99.904, 90.96799999023438, 99.956, 91.50399999023438, 99.964, 99.75200000488282, 98.34400000732421, 75.50000000976563, 99.988, 99.956, 91.21999999511719, 98.07200000976563, 99.98, 99.996, 99.6040000048828, 99.94, 99.932, 98.8320000024414, 99.30000000244141, 98.46000000976562, 96.00000001953126, 99.964, 99.924, 99.3920000024414, 99.976, 99.908, 99.916, 99.8440000024414, 99.92, 99.832, 99.984, 99.948, 99.192, 99.988, 99.84, 99.952, 99.98, 99.4920000024414, 99.928, 99.928, 98.33200000488281, 99.972, 99.98, 99.944, 99.932, 99.968, 99.972, 99.944, 99.928, 99.916, 99.748, 99.964, 99.972, 99.972, 99.98, 99.964, 99.98, 99.944, 99.892, 99.968, 99.912, 99.968, 99.956, 99.984, 99.96, 99.932, 99.976, 99.968, 99.98, 99.912, 99.868, 99.52, 99.988, 86.70399997802734, 99.96, 99.956, 99.916, 99.948, 99.992, 78.73199999023437, 99.98, 99.768]\n",
            "Validation accuracies: [83.83, 66.22, 88.09, 88.68, 89.14, 90.01, 85.91, 81.87, 66.3, 87.35, 90.1, 90.56, 90.95, 88.68, 89.93, 89.51, 88.33, 88.93, 86.58, 84.01, 88.94, 82.99, 88.91, 87.9, 84.97, 72.65, 90.33, 90.2, 84.73, 84.94, 90.38, 90.66, 87.6, 88.77, 88.94, 86.67, 87.15, 85.48, 85.33, 89.21, 88.85, 87.0, 90.23, 87.8, 88.55, 88.28, 88.67, 88.1, 89.48, 89.49, 86.46, 90.56, 88.29, 89.68, 88.75, 86.63, 89.22, 88.08, 86.62, 89.0, 88.65, 88.57, 89.3, 89.64, 88.0, 88.83, 88.35, 88.72, 87.86, 88.05, 89.37, 89.43, 89.08, 88.18, 89.06, 89.45, 87.31, 89.3, 88.83, 89.55, 89.33, 89.46, 89.0, 88.71, 89.62, 89.5, 88.65, 89.61, 87.77, 87.23, 90.33, 82.09, 89.56, 89.43, 88.45, 89.05, 90.24, 73.95, 88.85, 88.02]\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mloading model from file runs/Bananas/NasBench201SearchSpace/cifar10/333/search/model_final.pth\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (3, 2, 0, 0, 3, 2)\n",
            "\u001b[32m[10/08 11:53:13 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 90.95\n",
            "\u001b[32m[10/08 11:53:14 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs/Bananas/NasBench201SearchSpace/cifar10/444\n",
            "search:\n",
            "  acq_fn_optimization: random_sampling\n",
            "  acq_fn_type: its\n",
            "  checkpoint_freq: 100\n",
            "  encoding_type: path\n",
            "  epochs: 100\n",
            "  fidelity: -1\n",
            "  k: 20\n",
            "  max_mutations: 1\n",
            "  num_arches_to_mutate: 5\n",
            "  num_candidates: 100\n",
            "  num_ensemble: 1\n",
            "  num_init: 10\n",
            "  population_size: 30\n",
            "  predictor_type: xgb\n",
            "  sample_size: 10\n",
            "  seed: 444\n",
            "\u001b[32m[10/08 11:53:14 naslib]: \u001b[0mLoading Benchmark API\n",
            "\u001b[32m[10/08 11:53:15 nl.defaults.trainer]: \u001b[0mBeginning search\n",
            "\u001b[32m[10/08 11:53:16 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 96.48400, Validation accuracy: 84.99000\n",
            "\u001b[32m[10/08 11:53:16 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 99.98400, Validation accuracy: 89.59000\n",
            "\u001b[32m[10/08 11:53:16 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 99.96400, Validation accuracy: 89.77000\n",
            "\u001b[32m[10/08 11:53:16 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 99.97200, Validation accuracy: 89.29000\n",
            "\u001b[32m[10/08 11:53:17 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 99.80400, Validation accuracy: 86.00000\n",
            "\u001b[32m[10/08 11:53:17 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy: 99.76800, Validation accuracy: 88.02000\n",
            "\u001b[32m[10/08 11:53:17 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy: 99.96000, Validation accuracy: 90.07000\n",
            "\u001b[32m[10/08 11:53:18 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy: 99.92000, Validation accuracy: 89.08000\n",
            "\u001b[32m[10/08 11:53:18 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy: 100.00000, Validation accuracy: 91.15000\n",
            "\u001b[32m[10/08 11:53:18 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy: 79.54000, Validation accuracy: 74.85000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy: 99.98400, Validation accuracy: 89.78000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy: 99.82000, Validation accuracy: 85.67000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy: 99.98400, Validation accuracy: 90.45000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy: 99.96800, Validation accuracy: 90.21000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy: 99.97200, Validation accuracy: 90.32000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy: 99.86800, Validation accuracy: 88.34000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy: 99.93600, Validation accuracy: 89.16000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy: 99.97200, Validation accuracy: 90.22000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy: 99.88400, Validation accuracy: 89.64000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy: 91.44800, Validation accuracy: 82.42000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy: 98.88000, Validation accuracy: 86.40000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy: 80.97200, Validation accuracy: 71.87000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy: 90.77600, Validation accuracy: 81.19000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy: 97.95200, Validation accuracy: 85.98000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy: 98.85600, Validation accuracy: 86.15000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy: 99.27200, Validation accuracy: 85.33000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy: 99.91200, Validation accuracy: 89.09000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy: 99.81600, Validation accuracy: 87.45000\n",
            "\u001b[32m[10/08 11:53:40 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy: 99.48800, Validation accuracy: 87.33000\n",
            "\u001b[32m[10/08 11:53:41 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy: 99.14800, Validation accuracy: 86.85000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy: 99.96000, Validation accuracy: 89.96000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy: 99.96000, Validation accuracy: 87.39000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy: 99.94000, Validation accuracy: 87.70000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy: 99.91600, Validation accuracy: 88.01000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy: 99.94400, Validation accuracy: 88.90000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy: 99.79200, Validation accuracy: 87.42000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy: 99.98000, Validation accuracy: 90.46000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy: 99.85600, Validation accuracy: 88.64000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy: 100.00000, Validation accuracy: 91.18000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy: 99.34800, Validation accuracy: 84.49000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy: 99.99200, Validation accuracy: 90.08000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy: 96.65200, Validation accuracy: 84.93000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy: 10.28800, Validation accuracy: 10.00000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy: 99.96400, Validation accuracy: 89.09000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy: 99.90000, Validation accuracy: 88.49000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy: 99.96000, Validation accuracy: 88.74000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy: 99.98400, Validation accuracy: 90.14000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy: 99.64800, Validation accuracy: 85.01000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy: 99.86800, Validation accuracy: 88.48000\n",
            "\u001b[32m[10/08 11:54:01 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy: 99.86000, Validation accuracy: 82.25000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy: 99.96400, Validation accuracy: 90.07000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy: 99.99600, Validation accuracy: 89.43000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy: 96.41200, Validation accuracy: 85.53000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy: 99.90000, Validation accuracy: 88.65000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy: 98.94800, Validation accuracy: 85.72000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy: 99.91600, Validation accuracy: 88.76000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy: 99.98800, Validation accuracy: 89.23000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy: 99.96000, Validation accuracy: 88.64000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy: 99.97200, Validation accuracy: 88.00000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy: 99.97200, Validation accuracy: 88.34000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy: 99.93600, Validation accuracy: 88.72000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy: 99.96800, Validation accuracy: 89.95000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy: 99.96000, Validation accuracy: 89.20000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy: 99.94000, Validation accuracy: 86.50000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy: 99.94800, Validation accuracy: 88.00000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy: 99.94800, Validation accuracy: 87.67000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy: 99.98800, Validation accuracy: 88.73000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy: 99.96800, Validation accuracy: 88.48000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy: 99.97600, Validation accuracy: 89.95000\n",
            "\u001b[32m[10/08 11:54:23 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy: 99.84800, Validation accuracy: 87.11000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy: 99.98800, Validation accuracy: 90.67000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy: 99.97600, Validation accuracy: 90.13000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy: 99.72800, Validation accuracy: 88.23000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy: 80.66000, Validation accuracy: 71.37000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy: 99.98400, Validation accuracy: 89.47000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy: 99.94000, Validation accuracy: 89.00000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy: 97.25200, Validation accuracy: 85.73000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy: 99.91600, Validation accuracy: 88.69000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy: 99.73600, Validation accuracy: 87.74000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy: 99.92800, Validation accuracy: 88.91000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy: 99.50800, Validation accuracy: 87.54000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy: 100.00000, Validation accuracy: 90.63000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy: 99.97200, Validation accuracy: 90.13000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy: 99.98000, Validation accuracy: 90.55000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy: 99.97600, Validation accuracy: 88.63000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy: 98.96000, Validation accuracy: 83.06000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy: 99.92000, Validation accuracy: 88.69000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy: 99.93600, Validation accuracy: 86.98000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy: 99.94800, Validation accuracy: 88.76000\n",
            "\u001b[32m[10/08 11:54:43 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy: 99.96000, Validation accuracy: 88.62000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy: 99.98400, Validation accuracy: 90.75000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy: 99.97600, Validation accuracy: 89.76000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy: 99.95600, Validation accuracy: 89.26000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy: 99.92400, Validation accuracy: 88.62000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy: 99.97200, Validation accuracy: 89.90000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy: 99.96800, Validation accuracy: 89.22000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy: 91.42000, Validation accuracy: 84.02000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy: 99.93600, Validation accuracy: 89.51000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy: 99.93600, Validation accuracy: 88.08000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy: 99.82800, Validation accuracy: 86.56000\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [96.48400001708984, 99.984, 99.964, 99.972, 99.80400000244141, 99.768, 99.96, 99.92, 100.0, 79.53999997558594, 99.984, 99.8200000024414, 99.984, 99.968, 99.972, 99.868, 99.936, 99.972, 99.88400000488281, 91.44799999267578, 98.88000000488282, 80.97199997070312, 90.7760000024414, 97.95200000732422, 98.85600000732421, 99.272, 99.912, 99.816, 99.4880000048828, 99.14800000488282, 99.96, 99.96, 99.94, 99.916, 99.944, 99.792, 99.98, 99.856, 100.0, 99.34800000244141, 99.992, 96.65200001953124, 10.287999999389648, 99.964, 99.9, 99.96, 99.984, 99.648, 99.868, 99.8600000048828, 99.964, 99.996, 96.41200001708984, 99.9, 98.94800000488281, 99.916, 99.988, 99.96, 99.972, 99.972, 99.936, 99.968, 99.96, 99.94, 99.948, 99.948, 99.988, 99.968, 99.976, 99.848, 99.988, 99.976, 99.728, 80.65999997558593, 99.984, 99.94, 97.25200001220703, 99.916, 99.7360000024414, 99.928, 99.508, 100.0, 99.972, 99.98, 99.976, 98.96000000488282, 99.92000000244141, 99.936, 99.948, 99.96, 99.984, 99.976, 99.956, 99.924, 99.972, 99.968, 91.41999999267578, 99.936, 99.936, 99.8280000024414]\n",
            "Validation accuracies: [84.99, 89.59, 89.77, 89.29, 86.0, 88.02, 90.07, 89.08, 91.15, 74.85, 89.78, 85.67, 90.45, 90.21, 90.32, 88.34, 89.16, 90.22, 89.64, 82.42, 86.4, 71.87, 81.19, 85.98, 86.15, 85.33, 89.09, 87.45, 87.33, 86.85, 89.96, 87.39, 87.7, 88.01, 88.9, 87.42, 90.46, 88.64, 91.18, 84.49, 90.08, 84.93, 10.0, 89.09, 88.49, 88.74, 90.14, 85.01, 88.48, 82.25, 90.07, 89.43, 85.53, 88.65, 85.72, 88.76, 89.23, 88.64, 88.0, 88.34, 88.72, 89.95, 89.2, 86.5, 88.0, 87.67, 88.73, 88.48, 89.95, 87.11, 90.67, 90.13, 88.23, 71.37, 89.47, 89.0, 85.73, 88.69, 87.74, 88.91, 87.54, 90.63, 90.13, 90.55, 88.63, 83.06, 88.69, 86.98, 88.76, 88.62, 90.75, 89.76, 89.26, 88.62, 89.9, 89.22, 84.02, 89.51, 88.08, 86.56]\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mloading model from file runs/Bananas/NasBench201SearchSpace/cifar10/444/search/model_final.pth\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (2, 2, 0, 2, 2, 3)\n",
            "\u001b[32m[10/08 11:55:08 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 91.18\n",
            "\u001b[32m[10/08 11:55:09 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs/Bananas/NasBench201SearchSpace/cifar10/555\n",
            "search:\n",
            "  acq_fn_optimization: random_sampling\n",
            "  acq_fn_type: its\n",
            "  checkpoint_freq: 100\n",
            "  encoding_type: path\n",
            "  epochs: 100\n",
            "  fidelity: -1\n",
            "  k: 20\n",
            "  max_mutations: 1\n",
            "  num_arches_to_mutate: 5\n",
            "  num_candidates: 100\n",
            "  num_ensemble: 1\n",
            "  num_init: 10\n",
            "  population_size: 30\n",
            "  predictor_type: xgb\n",
            "  sample_size: 10\n",
            "  seed: 555\n",
            "\u001b[32m[10/08 11:55:09 naslib]: \u001b[0mLoading Benchmark API\n",
            "\u001b[32m[10/08 11:55:10 nl.defaults.trainer]: \u001b[0mBeginning search\n",
            "\u001b[32m[10/08 11:55:11 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 97.52400, Validation accuracy: 84.15000\n",
            "\u001b[32m[10/08 11:55:11 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 99.90400, Validation accuracy: 87.58000\n",
            "\u001b[32m[10/08 11:55:11 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 99.90800, Validation accuracy: 88.19000\n",
            "\u001b[32m[10/08 11:55:11 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 99.24800, Validation accuracy: 86.13000\n",
            "\u001b[32m[10/08 11:55:11 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 98.46400, Validation accuracy: 84.65000\n",
            "\u001b[32m[10/08 11:55:11 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy: 91.12800, Validation accuracy: 71.99000\n",
            "\u001b[32m[10/08 11:55:11 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy: 89.96400, Validation accuracy: 82.45000\n",
            "\u001b[32m[10/08 11:55:12 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy: 99.90800, Validation accuracy: 86.87000\n",
            "\u001b[32m[10/08 11:55:12 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy: 97.37600, Validation accuracy: 79.70000\n",
            "\u001b[32m[10/08 11:55:12 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy: 91.12800, Validation accuracy: 83.75000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy: 99.95600, Validation accuracy: 88.28000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy: 99.64800, Validation accuracy: 86.59000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy: 99.94000, Validation accuracy: 87.34000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy: 99.82800, Validation accuracy: 88.34000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy: 99.28800, Validation accuracy: 85.08000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy: 98.11200, Validation accuracy: 82.75000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy: 76.02000, Validation accuracy: 72.17000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy: 80.66000, Validation accuracy: 77.02000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy: 70.35600, Validation accuracy: 67.97000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy: 99.92400, Validation accuracy: 88.89000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy: 99.95600, Validation accuracy: 89.21000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy: 99.98000, Validation accuracy: 90.53000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy: 99.95600, Validation accuracy: 88.71000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy: 99.93600, Validation accuracy: 88.08000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy: 99.96800, Validation accuracy: 89.78000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy: 90.78800, Validation accuracy: 84.23000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy: 99.06400, Validation accuracy: 85.51000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy: 99.91600, Validation accuracy: 89.73000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy: 99.98800, Validation accuracy: 88.95000\n",
            "\u001b[32m[10/08 11:55:37 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy: 99.76400, Validation accuracy: 87.65000\n",
            "\u001b[32m[10/08 11:55:58 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy: 99.91600, Validation accuracy: 88.98000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy: 99.98800, Validation accuracy: 90.73000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy: 99.99200, Validation accuracy: 90.45000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy: 99.94400, Validation accuracy: 88.88000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy: 99.96400, Validation accuracy: 88.91000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy: 99.84800, Validation accuracy: 88.22000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy: 99.93200, Validation accuracy: 88.89000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy: 99.97600, Validation accuracy: 88.83000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy: 99.96000, Validation accuracy: 89.29000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy: 99.96800, Validation accuracy: 89.10000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy: 99.95200, Validation accuracy: 88.38000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy: 99.98800, Validation accuracy: 89.97000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy: 93.56000, Validation accuracy: 79.23000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy: 99.95600, Validation accuracy: 89.04000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy: 99.95200, Validation accuracy: 88.43000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy: 99.91600, Validation accuracy: 87.94000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy: 99.88000, Validation accuracy: 88.67000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy: 99.91600, Validation accuracy: 88.83000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy: 99.02800, Validation accuracy: 86.12000\n",
            "\u001b[32m[10/08 11:55:59 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy: 99.94400, Validation accuracy: 88.74000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy: 99.95600, Validation accuracy: 89.46000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy: 99.98000, Validation accuracy: 90.46000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy: 99.97600, Validation accuracy: 89.80000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy: 99.97600, Validation accuracy: 89.78000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy: 99.94800, Validation accuracy: 89.45000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy: 99.98400, Validation accuracy: 90.64000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy: 99.96800, Validation accuracy: 89.36000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy: 99.96400, Validation accuracy: 89.66000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy: 99.95200, Validation accuracy: 89.06000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy: 99.96400, Validation accuracy: 89.54000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy: 99.97600, Validation accuracy: 89.52000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy: 99.91200, Validation accuracy: 88.32000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy: 94.21600, Validation accuracy: 83.70000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy: 99.55200, Validation accuracy: 86.80000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy: 98.99600, Validation accuracy: 86.33000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy: 99.55600, Validation accuracy: 87.42000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy: 99.90400, Validation accuracy: 86.83000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy: 99.94000, Validation accuracy: 88.43000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy: 99.99200, Validation accuracy: 90.78000\n",
            "\u001b[32m[10/08 11:56:19 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy: 99.78800, Validation accuracy: 88.36000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy: 99.98800, Validation accuracy: 90.51000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy: 99.98400, Validation accuracy: 90.87000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy: 99.96000, Validation accuracy: 89.58000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy: 99.95600, Validation accuracy: 89.65000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy: 99.97600, Validation accuracy: 90.28000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy: 99.94800, Validation accuracy: 89.12000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy: 99.96400, Validation accuracy: 88.93000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy: 74.78800, Validation accuracy: 71.93000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy: 99.29600, Validation accuracy: 87.20000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy: 99.97600, Validation accuracy: 90.48000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy: 99.60000, Validation accuracy: 87.52000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy: 99.96400, Validation accuracy: 88.26000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy: 99.91200, Validation accuracy: 88.14000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy: 99.94400, Validation accuracy: 88.88000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy: 99.92800, Validation accuracy: 88.63000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy: 91.70400, Validation accuracy: 82.60000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy: 99.93600, Validation accuracy: 88.43000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy: 99.96000, Validation accuracy: 88.17000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy: 99.86800, Validation accuracy: 88.34000\n",
            "\u001b[32m[10/08 11:56:39 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy: 99.86800, Validation accuracy: 88.34000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy: 99.98400, Validation accuracy: 90.83000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy: 99.98000, Validation accuracy: 89.67000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy: 99.92000, Validation accuracy: 88.59000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy: 99.95600, Validation accuracy: 89.58000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy: 99.95600, Validation accuracy: 89.11000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy: 99.83200, Validation accuracy: 88.59000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy: 99.96800, Validation accuracy: 89.83000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy: 99.94000, Validation accuracy: 89.00000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy: 99.89600, Validation accuracy: 88.98000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy: 99.96400, Validation accuracy: 89.16000\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [97.52400001464844, 99.904, 99.908, 99.2480000024414, 98.4640000048828, 91.12800000488281, 89.96399998535156, 99.908, 97.37600000976562, 91.12799998535156, 99.956, 99.6480000024414, 99.94, 99.8280000024414, 99.28800000732421, 98.11200001220703, 76.02000000488282, 80.65999997314454, 70.35599998779297, 99.9240000024414, 99.956, 99.98, 99.956, 99.936, 99.968, 90.78799998779297, 99.0640000024414, 99.916, 99.988, 99.764, 99.916, 99.988, 99.992, 99.944, 99.964, 99.848, 99.93200000244141, 99.976, 99.96, 99.968, 99.952, 99.988, 93.55999997314453, 99.956, 99.952, 99.916, 99.88, 99.916, 99.0280000024414, 99.944, 99.9560000024414, 99.98, 99.976, 99.976, 99.948, 99.984, 99.9680000024414, 99.964, 99.952, 99.964, 99.976, 99.912, 94.21600001953125, 99.55200000732422, 98.99600001220703, 99.55600000488282, 99.904, 99.94, 99.992, 99.788, 99.988, 99.984, 99.96, 99.956, 99.976, 99.948, 99.964, 74.78799998046875, 99.29600000976562, 99.976, 99.60000000488282, 99.964, 99.912, 99.944, 99.928, 91.70399998046875, 99.936, 99.96, 99.868, 99.868, 99.984, 99.98, 99.92, 99.956, 99.956, 99.8320000024414, 99.968, 99.94, 99.896, 99.96400000244141]\n",
            "Validation accuracies: [84.15, 87.58, 88.19, 86.13, 84.65, 71.99, 82.45, 86.87, 79.7, 83.75, 88.28, 86.59, 87.34, 88.34, 85.08, 82.75, 72.17, 77.02, 67.97, 88.89, 89.21, 90.53, 88.71, 88.08, 89.78, 84.23, 85.51, 89.73, 88.95, 87.65, 88.98, 90.73, 90.45, 88.88, 88.91, 88.22, 88.89, 88.83, 89.29, 89.1, 88.38, 89.97, 79.23, 89.04, 88.43, 87.94, 88.67, 88.83, 86.12, 88.74, 89.46, 90.46, 89.8, 89.78, 89.45, 90.64, 89.36, 89.66, 89.06, 89.54, 89.52, 88.32, 83.7, 86.8, 86.33, 87.42, 86.83, 88.43, 90.78, 88.36, 90.51, 90.87, 89.58, 89.65, 90.28, 89.12, 88.93, 71.93, 87.2, 90.48, 87.52, 88.26, 88.14, 88.88, 88.63, 82.6, 88.43, 88.17, 88.34, 88.34, 90.83, 89.67, 88.59, 89.58, 89.11, 88.59, 89.83, 89.0, 88.98, 89.16]\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mloading model from file runs/Bananas/NasBench201SearchSpace/cifar10/555/search/model_final.pth\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (2, 3, 0, 2, 4, 3)\n",
            "\u001b[32m[10/08 11:56:59 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 90.87\n"
          ]
        }
      ],
      "source": [
        "# Now lets use BANANAS as a search optimizer with xgb as predictor\n",
        "# Note: We are fitting a predictor preriodicaly so this is expected to be slower than RE and RS\n",
        "from naslib.search_spaces.core.query_metrics import Metric\n",
        "from naslib import utils\n",
        "from naslib.utils import get_dataset_api\n",
        "from naslib.utils.logging import setup_logger\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from naslib.defaults.trainer import Trainer\n",
        "\n",
        "\n",
        "from fvcore.common.config import CfgNode\n",
        "optimizer_type = Bananas # {RegularizedEvolution, RandomSearch}\n",
        "search_space_type = NasBench201SearchSpace # {NasBench101SearchSpace, NasBench201SearchSpace, NasBench301SearchSpace}\n",
        "\n",
        "# Set the dataset\n",
        "dataset = 'cifar10' # cifar10 for NB101 and NB301, {cifar100, ImageNet16-120} for NB201\n",
        "\n",
        "#define the predictor\n",
        "predictor = 'xgb' \n",
        "for seed in [333,444,555]:\n",
        "  config = {\n",
        "    'search': {\n",
        "        # Required by Trainer\n",
        "        'epochs': 100,\n",
        "        'checkpoint_freq': 100,\n",
        "        \n",
        "        # Required by Random Search optimizer\n",
        "        'fidelity': -1,\n",
        "        'k':20,\n",
        "        \"num_init\":10,\n",
        "        \"num_ensemble\":1,\n",
        "        # Required by RegularizedEvolution\n",
        "        'sample_size': 10,\n",
        "        'population_size': 30,\n",
        "        \"predictor_type\": predictor,\n",
        "        \"acq_fn_type\": \"its\",\n",
        "        \"acq_fn_optimization\": 'random_sampling',\n",
        "        \"encoding_type\": 'path',\n",
        "        \"num_arches_to_mutate\":5,\n",
        "        \"max_mutations\":1,\n",
        "        \"num_candidates\":100\n",
        "     }\n",
        "   }\n",
        "  config = CfgNode.load_cfg(json.dumps(config))\n",
        "\n",
        "  search_trajectory, best_model, best_model_val_acc = run_optimizer(\n",
        "                                                        optimizer_type,\n",
        "                                                        search_space_type,\n",
        "                                                        dataset,\n",
        "                                                        config,\n",
        "                                                        seed\n",
        "                                                    )\n",
        "  if \"BANANAS_\"+str(predictor) in trajectories_bn.keys():\n",
        "    trajectories_bn[\"BANANAS_\"+str(predictor)].append(search_trajectory)\n",
        "  else: \n",
        "    trajectories_bn[\"BANANAS_\"+str(predictor)] = []\n",
        "    trajectories_bn[\"BANANAS_\"+str(predictor)].append(search_trajectory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "u8F7JZOhE_vk",
        "outputId": "b66b6d11-fb8c-42d5-9f18-0a86235fd96a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot Bananas (xgb predictor), RS and RE\n",
        "plot_optimizers(trajectories_bn,\"NAS-Bench-201 cifar10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v63Ld4lTZDj4"
      },
      "source": [
        "### TASK 3: BANANAS with predictor of your choice\n",
        "1. Choose any predictor from amongst `'mlp','lgb', 'rf','bayes_lin_reg','gp'` and run search using BANANAS\n",
        "2. Use NASBench201 search space for the ImageNet16-120 dataset\n",
        "3. Plot your results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXukzHgWaMrU"
      },
      "outputs": [],
      "source": [
        "trajectories_bn_imgnet={}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4mN5dczkRSVw"
      },
      "outputs": [],
      "source": [
        "# Bananas with any other predictor of your choice\n",
        "from naslib.search_spaces.core.query_metrics import Metric\n",
        "from naslib import utils\n",
        "from naslib.utils import get_dataset_api\n",
        "from naslib.utils.logging import setup_logger\n",
        "import os\n",
        "import logging\n",
        "import sys\n",
        "from naslib.defaults.trainer import Trainer\n",
        "from naslib.optimizers import (\n",
        "    DARTSOptimizer,\n",
        "    GDASOptimizer,\n",
        "    DrNASOptimizer,\n",
        "    RandomSearch,\n",
        "    RegularizedEvolution,\n",
        "    LocalSearch,\n",
        "    Bananas,\n",
        "    BasePredictor,\n",
        ")\n",
        "import json\n",
        "from naslib.search_spaces import (\n",
        "    NasBench301SearchSpace,\n",
        "    SimpleCellSearchSpace,\n",
        "    NasBench201SearchSpace,\n",
        "    HierarchicalSearchSpace,\n",
        ")\n",
        "\n",
        "from fvcore.common.config import CfgNode\n",
        "optimizer_type = Bananas # {RegularizedEvolution, RandomSearch}\n",
        "search_space_type = ... # Specify the search space\n",
        "\n",
        "# Set the dataset\n",
        "dataset = ... # Specify the dataset\n",
        "\n",
        "predictor = ... # Specify the predictor\n",
        "for seed in [333,444,555]:\n",
        "  config = {\n",
        "    'search': {\n",
        "        # Required by Trainer\n",
        "        'epochs': 100,\n",
        "        'checkpoint_freq': 100,\n",
        "        \n",
        "        # Required by Random Search optimizer\n",
        "        'fidelity': -1,\n",
        "        'k':20,\n",
        "        \"num_init\":10,\n",
        "        \"num_ensemble\":1,\n",
        "        # Required by RegularizedEvolution\n",
        "        'sample_size': 10,\n",
        "        'population_size': 30,\n",
        "        \"predictor_type\": predictor,\n",
        "        \"acq_fn_type\": \"its\",\n",
        "        \"acq_fn_optimization\": 'random_sampling',\n",
        "        \"encoding_type\": 'path',\n",
        "        \"num_arches_to_mutate\":5,\n",
        "        \"max_mutations\":1,\n",
        "        \"num_candidates\":100\n",
        "     }\n",
        "   }\n",
        "  config = CfgNode.load_cfg(json.dumps(config))\n",
        "\n",
        "  search_trajectory, best_model, best_model_val_acc = run_optimizer(\n",
        "                                                        optimizer_type,\n",
        "                                                        search_space_type,\n",
        "                                                        dataset,\n",
        "                                                        '',\n",
        "                                                        config,\n",
        "                                                        seed\n",
        "                                                    )\n",
        "  if predictor in trajectories_bn_imgnet.keys():\n",
        "    trajectories_bn_imgnet[predictor].append(search_trajectory)\n",
        "  else: \n",
        "    trajectories_bn_imgnet[predictor] = []\n",
        "    trajectories_bn_imgnet[predictor].append(search_trajectory)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o0UfkSi8RlUW"
      },
      "outputs": [],
      "source": [
        "plot_optimizers(trajectories_bn_imgnet,'NAS-Bench201_ImageNet16-120')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHe1D65nb-hm"
      },
      "source": [
        "<a name=\"zcps\"></a>\n",
        "# 4. Zero Cost Proxies\n",
        "\n",
        "Zero Cost proxies allow us to *rank* models using only a single forward and backward pass of the data. Since they require much less time and compute to score models (as opposed to training them from scratch and comparing them, for example), they can be used as a cheap *proxy* to speed up black-box optimizers.\n",
        "\n",
        "In this section, we look how we can use one of the 13 Zero Cost proxies available in NASLib to *score* a model. Intuitively, the higher the score of a model, the better we expect the model to perform (this isn't always true, though - there are cases where the scores are anti-correlated with the validation performance).\n",
        "\n",
        "A perfect Zero Cost proxy would be one where its scores for models are perfectly correlated with the performance of those models, i.e., the ranking of the models by the Zero Cost proxies are the same as their ranking after they have been fully trained and evaluated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "1794cb3d6bda42738ce9f9dfa01e9b5c",
            "bdc2c14d710d431b9095153276bb27df",
            "98afd2dc6e9d4537b50d4ef04cf77a9f",
            "0d4a67abfa2e4eafa5a7c8cd1ce0c5c0",
            "aeea45fc34d04f62af72063e3fa83569",
            "b80c8d7c85de4e6d9e6476e6541d7e41",
            "e9c9e94559394e1bbf170c5e01d8299a",
            "c22b9e474e154ad68f12a87b686a5b68",
            "84aa3f6831e2460896f5612df7f97170",
            "e459c75405454a448dacf6fd02ac4fc6",
            "163f1d9f796d49a8931ce310de520917"
          ]
        },
        "id": "48bzvH8GGhUZ",
        "outputId": "6102cf89-13bc-4307-fe46-40a6669e0805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/NASLib/naslib/data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1794cb3d6bda42738ce9f9dfa01e9b5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /content/NASLib/naslib/data/cifar-10-python.tar.gz to /content/NASLib/naslib/data\n",
            "Files already downloaded and verified\n",
            "Score of model for Zero Cost predictor plain: 0.09085417538881302\n"
          ]
        }
      ],
      "source": [
        "from naslib.predictors import ZeroCost\n",
        "from naslib.search_spaces import NasBench201SearchSpace\n",
        "from naslib.utils import get_train_val_loaders, get_project_root\n",
        "from fvcore.common.config import CfgNode\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create configs required for get_train_val_loaders\n",
        "config = {\n",
        "    'dataset': 'cifar10', # Dataset to loader: can be cifar100, svhn, ImageNet16-120, jigsaw, class_object, class_scene, or autoencoder (the last four are TNB101 datasets)\n",
        "    'data': str(get_project_root()) + '/data', # path to naslib/data\n",
        "    'search': {\n",
        "        'seed': 9001, # Seed to use in the train, validation and test dataloaders\n",
        "        'train_portion': 0.7, # Portion of train dataset to use as train dataset. The rest is used as validation dataset.\n",
        "        'batch_size': 32, # batch size of the dataloaders\n",
        "    }\n",
        "}\n",
        "config = CfgNode(config)\n",
        "\n",
        "# Get the dataloaders\n",
        "train_loader, val_loader, test_loader, train_transform, valid_transform = get_train_val_loaders(config)\n",
        "\n",
        "# Sample a random NB201 graph and instantiate it\n",
        "graph = NasBench201SearchSpace()\n",
        "graph.sample_random_architecture()\n",
        "graph.parse()\n",
        "\n",
        "# Instantiate the ZeroCost predictor\n",
        "# The Zero Cost predictors can be any of the following:\n",
        "# {'epe_nas', 'fisher', 'grad_norm', 'grasp', 'jacov', 'l2_norm', 'nwot', 'plain', 'snip', 'synflow', 'zen', 'flops', 'params'}\n",
        "\n",
        "zc_pred = 'plain'\n",
        "zc_predictor = ZeroCost(method_type=zc_pred)\n",
        "score = zc_predictor.query(graph=graph, dataloader=train_loader)\n",
        "\n",
        "print(f'Score of model for Zero Cost predictor {zc_pred}: {score}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FhPnsO7yUpA"
      },
      "source": [
        "Let's now try scoring the same model with all the Zero Cost proxies. Remember that it only makes sense to compare between the scores of different models of a given proxy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VnXFul5O2M9",
        "outputId": "15fe9f75-5870-4f8c-b6fa-627afe563d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scores of model\n",
            "epe_nas: 164.107765\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fisher: 0.016750\n",
            "grad_norm: 4.093495\n",
            "grasp: -0.155474\n",
            "jacov: -732.195878\n",
            "l2_norm: 78.622498\n",
            "nwot: 305.624134\n",
            "plain: 0.115587\n",
            "snip: 5.831100\n",
            "synflow: 25.187156\n",
            "zen: 25.058960\n",
            "flops: 14.886592\n",
            "params: 0.101306\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the ZeroCost predictor\n",
        "# The Zero Cost predictors can be any of the following:\n",
        "zc_predictors = ['epe_nas', 'fisher', 'grad_norm', 'grasp', 'jacov', 'l2_norm', 'nwot', 'plain', 'snip', 'synflow', 'zen', 'flops', 'params']\n",
        "\n",
        "print('Scores of model')\n",
        "for zc_pred in zc_predictors:\n",
        "    zc_predictor = ZeroCost(method_type=zc_pred)\n",
        "    score = zc_predictor.query(graph=graph, dataloader=train_loader)\n",
        "    print(f'{zc_pred}: {score:3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1yEdOwyuuM"
      },
      "source": [
        "Now that we have seen all the Zero Cost proxies in action, let's compute the *rank correlation* between the proxy scores of a few models and their validation performance. The higher the correlation, the better the proxy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvfAWqRqO2sC"
      },
      "outputs": [],
      "source": [
        "from naslib.search_spaces.core import Metric\n",
        "from naslib.utils import compute_scores, get_dataset_api\n",
        "\n",
        "# Sample 50 random architectures, query their performances\n",
        "n_graphs = 50\n",
        "models = []\n",
        "val_accs = []\n",
        "zc_scores = []\n",
        "\n",
        "print('Loading NAS-Bench-201 API...')\n",
        "api = get_dataset_api('nasbench201', 'cifar10')\n",
        "\n",
        "print(f'Sampling {n_graphs} NAS-Bench-201 models...')\n",
        "for _ in tqdm(range(n_graphs)):\n",
        "    graph = NasBench201SearchSpace()\n",
        "    graph.sample_random_architecture()\n",
        "    graph.parse()\n",
        "\n",
        "    models.append(graph)\n",
        "\n",
        "print('Querying validation performance for all models')\n",
        "for graph in tqdm(models):\n",
        "    acc = graph.query(metric=Metric.VAL_ACCURACY, dataset='cifar10', dataset_api=api)\n",
        "    val_accs.append(acc)\n",
        "\n",
        "\n",
        "print('Scoring the models using Zero Cost predictor (jacov)')\n",
        "zc_predictor = ZeroCost(method_type='jacov')\n",
        "\n",
        "########## START TODO ############\n",
        "\n",
        "# Score the models\n",
        "# Add the scores to zc_scores \n",
        "\n",
        "########### END TODO #############\n",
        "\n",
        "# We now compute the correlation between val_accs (ground truth) and zc_scores (proxy scores)\n",
        "correlations = compute_scores(ytest=val_accs, test_pred=zc_scores)\n",
        "\n",
        "# Extract the results\n",
        "kendalltau_corr = correlations['kendalltau']\n",
        "spearman_corr = correlations['spearman']\n",
        "pearson_corr = correlations['pearson']\n",
        "\n",
        "print('*'*50)\n",
        "print('Validation accuracies: ', val_accs)\n",
        "print()\n",
        "print('Zero Cost predictor scores: ', zc_scores)\n",
        "print('*'*50)\n",
        "print('Correlations between validation accuracies (ground truth) and Zero Cost predictor scores (prediction): ')\n",
        "print('Kendall Tau correlation:', kendalltau_corr)\n",
        "print('Spearman correlation:', spearman_corr)\n",
        "print('Pearson correlation:', pearson_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uttJn87O3TwO"
      },
      "source": [
        "## Zero Cost Proxy Benchmarks\n",
        "\n",
        "Much like NAS-Benchmark APIs, we have introduced Zero Cost Benchmark APIs, which lets you query instantly the Zero Cost proxy scores for a given model in a search space. Once again, the idea is to speed-up NAS research. If a black-box optimizer you want to write leverages Zero Cost proxies, for example, you could leverage the Zero Cost Benchmarks to run the experiments much faster by simply querying them for the scores instead of the scoring the models using the Zero Cost proxies.\n",
        "\n",
        "We have already downloaded the Zero Cost Benchmark API for NAS-Bench-201, which has the scores for all 15625 models evaluated for all three datasets (CIFAR-10, CIFAR-100 and ImageNet16-120) using all 13 Zero Cost proxies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CxD8Ai5qn7"
      },
      "source": [
        "Let's now query the benchmark API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKYmVhYePYDK",
        "outputId": "98f1161f-f386-4d41-c1af-3ca34720a368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All the data available in the Zero Cost benchmark for model (2, 0, 0, 3, 0, 0): \n",
            "{'score': -0.016125619411468506, 'time': 2.279462897218764}\n",
            "Score of model with spec (2, 0, 0, 3, 0, 0) for Zero Cost proxies grasp: -0.016125619411468506\n",
            "Time taken to compute the score for the model: 2.28s\n"
          ]
        }
      ],
      "source": [
        "from naslib.utils import get_zc_benchmark_api\n",
        "\n",
        "# Load the Zero Cost Benchmark API for NAS-Bench-201 CIFAR-10\n",
        "zc_api = get_zc_benchmark_api('nasbench201', 'cifar10')\n",
        "graph = models[0]\n",
        "\n",
        "# Use the Zero Cost Benchmark to get the score for the model for a particular ZC proxy\n",
        "pred = 'grasp'\n",
        "spec = graph.get_hash()\n",
        "score = zc_api[str(spec)][pred]['score']\n",
        "time_to_compute = zc_api[str(spec)][pred]['time']\n",
        "\n",
        "print(f'All the data available in the Zero Cost benchmark for model {spec}: ')\n",
        "print(zc_api[str(spec)][pred])\n",
        "print(f'Score of model with spec {spec} for Zero Cost proxies {pred}: {score}')\n",
        "print(f'Time taken to compute the score for the model: {time_to_compute:.2f}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2t9jAOrPc_4",
        "outputId": "66c023b6-2d10-4fc7-8172-fd449c3e2942"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:00<00:00, 8909.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zc_scores: {'epe_nas': [], 'fisher': [], 'grad_norm': [], 'grasp': [], 'jacov': [], 'l2_norm': [], 'nwot': [], 'plain': [], 'snip': [], 'synflow': [], 'zen': [], 'flops': [], 'params': []}\n",
            "Querying Zero Cost Benchmark for scores\n",
            "epe_nas [1292.8213521234534, 1677.4133181362522, 771.6244256973021, 1737.454440152671, 1573.3680969091486]\n",
            "fisher [0.0032408733386546373, 0.6351345181465149, 0.036273304373025894, 0.04645204544067383, 1.5173672437667847]\n",
            "grad_norm [1.9650213718414307, 72.5101547241211, 7.580556869506836, 16.29119300842285, 126.30833435058594]\n",
            "grasp [-0.016125619411468506, -3.2642059326171875, -0.06577873229980469, 4.319438457489014, 157.290771484375]\n",
            "jacov [-72.90912838633159, -65.15828505076317, -311.63863660167726, -65.98522306098099, -65.11022856424916]\n",
            "l2_norm [129.5747528076172, 231.27603149414062, 78.764892578125, 180.98062133789062, 282.43365478515625]\n",
            "nwot [756.8241446485636, 789.6434823031072, 611.1394665263265, 782.2980597435335, 818.9739002527507]\n",
            "plain [0.06447502225637436, 0.11616043746471405, 0.23372536897659302, 0.17150962352752686, -0.022408679127693176]\n",
            "snip [3.450115919113159, 117.0306396484375, 16.48707389831543, 35.3172721862793, 161.0484161376953]\n",
            "synflow [68.96618744961702, 65.32086886367614, 14.727328236083682, 49.8678214509082, 85.10796542468181]\n",
            "zen [149.5738067626953, 63.220680236816406, 6.479037284851074, 51.21070098876953, 105.7850341796875]\n",
            "flops [48.125632, 58.140352, 45.053632, 22.320832, 93.959872]\n",
            "params [0.344346, 0.400346, 0.316346, 0.157306, 0.643386]\n",
            "Kendall-Tau correlations for all the predictors:\n",
            "{'epe_nas': 0.34530612244897957, 'fisher': 0.38285714285714284, 'grad_norm': 0.43673469387755104, 'grasp': 0.36979591836734693, 'jacov': 0.48244897959183675, 'l2_norm': 0.5510204081632653, 'nwot': 0.5853061224489795, 'plain': -0.40244897959183673, 'snip': 0.43673469387755104, 'synflow': 0.58146186962603, 'zen': 0.2816326530612245, 'flops': 0.4946655921380456, 'params': 0.5210917321575367}\n",
            "ZC predictors ranked (best to worst):\n",
            "#1 nwot 0.5853061224489795\n",
            "#2 synflow 0.58146186962603\n",
            "#3 l2_norm 0.5510204081632653\n",
            "#4 params 0.5210917321575367\n",
            "#5 flops 0.4946655921380456\n",
            "#6 jacov 0.48244897959183675\n",
            "#7 grad_norm 0.43673469387755104\n",
            "#8 snip 0.43673469387755104\n",
            "#9 fisher 0.38285714285714284\n",
            "#10 grasp 0.36979591836734693\n",
            "#11 epe_nas 0.34530612244897957\n",
            "#12 zen 0.2816326530612245\n",
            "#13 plain -0.40244897959183673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "zc_scores = {pred: [] for pred in zc_predictors} # Just dictionary of an empty list for each predictor\n",
        "print('zc_scores:', zc_scores)\n",
        "\n",
        "print('Querying Zero Cost Benchmark for scores')\n",
        "for graph in tqdm(models):\n",
        "    spec = graph.get_hash()\n",
        "\n",
        "    # Get the score for this model for all the Zero Cost proxies\n",
        "    for pred in zc_predictors:\n",
        "        score = zc_api[str(spec)][pred]['score']\n",
        "        zc_scores[pred].append (score)\n",
        "\n",
        "# Print the Zero Cost values for the first 5 models\n",
        "for pred in zc_predictors:\n",
        "    print(pred, zc_scores[pred][:5])\n",
        "\n",
        "kt_corrs = {}\n",
        "# Compute the rank correlation for each of the predictors\n",
        "for pred in zc_predictors:\n",
        "    correlations = compute_scores(ytest=val_accs, test_pred=zc_scores[pred])\n",
        "    kendalltau_corr = correlations['kendalltau']\n",
        "    kt_corrs[pred] = kendalltau_corr\n",
        "\n",
        "print('Kendall-Tau correlations for all the predictors:')\n",
        "print(kt_corrs)\n",
        "# Sort the predictors from best to worst for these models\n",
        "kt_corrs_list = [(pred, score) for pred, score in kt_corrs.items()]\n",
        "kt_corrs_list = sorted(kt_corrs_list, reverse=True, key=lambda pred_score_tuple: pred_score_tuple[1])\n",
        "\n",
        "print('ZC predictors ranked (best to worst):')\n",
        "for idx, (pred, score) in enumerate(kt_corrs_list):\n",
        "    print(f'#{idx+1}', pred, score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4Doo29jFoaO"
      },
      "source": [
        "# Bonus Challenge: Awesome Optimizer\n",
        "\n",
        "Blazed through the tutorial? Want an extra challenge?\n",
        "\n",
        "We ask you to write a black box optimizer that beats the RandomSearch baseline for NAS-Bench-201 for CIFAR-10, CIFAR-100 and ImageNet16-120 datasets.\n",
        "\n",
        "Given below is the skeletal code for a NASLib Black-box optimizer, with everything except its main method - new_epoch() - filled. This method is called in every step of the search of the Trainer (as seen in the run_optimizer function before) and is responsible for executing the main logic of the optimizer. E.g., the Random-Search optimizer samples a random architecture and queries its validation performance here.\n",
        "\n",
        "Remember that every query of a NAS-Benchmark for its validation performance is a substitute for training the model from scratch and evaluating its performance.\n",
        "\n",
        "In this challenge, there is only one rule: **You may query the validation performance of only ONE model in new_epoch()**. \n",
        "\n",
        "i.e., only one model shall be evaluated in every call of new_epoch(). Feel free to sample several models, and rank them, and query only the best model, for example. You are free to implement your own ideas here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "p1Uod8DFFxMl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from fvcore.common.config import CfgNode\n",
        "\n",
        "from naslib.optimizers.core.metaclasses import MetaOptimizer\n",
        "from naslib.predictors.zerocost import ZeroCost\n",
        "from naslib.search_spaces.core.graph import Graph\n",
        "from naslib.search_spaces.core.query_metrics import Metric\n",
        "from naslib.utils import get_train_val_loaders\n",
        "\n",
        "# Import whatever else you want from NASLib\n",
        "\n",
        "class AwesomeOptimizer(MetaOptimizer):\n",
        "    \"\"\"\n",
        "    Implement your awesome optimizer here.\n",
        "\n",
        "    This optimizer inherits from RandomSearch purely for convenience. Your search\n",
        "    method does not have to be random at all. Feel free to write your own logic, and add \n",
        "    any new methods that you need for it.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: CfgNode):\n",
        "        \"\"\"\n",
        "        Initialize your optimizer\n",
        "\n",
        "        Args:\n",
        "            config\n",
        "        \"\"\"\n",
        "        super(AwesomeOptimizer, self).__init__()\n",
        "        # These two lists are required\n",
        "        self.sampled_archs = []\n",
        "        self.history = []\n",
        "        \n",
        "        self.performance_metric = Metric.VAL_ACCURACY\n",
        "        # You can add more properties to the config file and read them if you want\n",
        "        self.dataset = config.dataset\n",
        "        self.fidelity = config.search.fidelity\n",
        "\n",
        "        # A few things that might be useful to you. Add/remove code as you wish.\n",
        "        self.trainloader = get_train_val_loaders(config)[0]\n",
        "        self.zerocostpredictor = ZeroCost(method_type='l2_norm')\n",
        "        # self.my_awesome_property = config.my_awesome_property\n",
        "\n",
        "        ###########################################################\n",
        "        ##################### START TODO ##########################\n",
        "\n",
        "\n",
        "        # Need more stuff in your initializer? Write them here!\n",
        "\n",
        "\n",
        "        ##################### END TODO  ##########################\n",
        "        ##########################################################\n",
        "\n",
        "\n",
        "    def adapt_search_space(self, search_space: Graph, scope: str = None, dataset_api: dict = None):\n",
        "        \"\"\"\n",
        "        This method has to be called with the search_space and the nas benchmark api before the optimizer\n",
        "        can be used.\n",
        "\n",
        "        Args:\n",
        "            search_space : An instance of the search space, such as NasBench201SearchSpace()\n",
        "            scope        : Relevant only for one-shot optimizers\n",
        "            dataset_api  : NAS Benchmark API for the given search space\n",
        "        \"\"\"\n",
        "        self.search_space = search_space.clone()\n",
        "        self.dataset_api = dataset_api\n",
        "\n",
        "\n",
        "    def new_epoch(self, epoch: int):\n",
        "        \"\"\"\n",
        "        This method is called in every \"step\" of the search.\n",
        "\n",
        "        Args:\n",
        "            epoch: epoch number\n",
        "        \"\"\"\n",
        "\n",
        "        # For reference, this is the code inside new_epoch in RandomSearch:\n",
        "\n",
        "        # model = torch.nn.Module()\n",
        "        # model.arch = self.search_space.clone()\n",
        "        # model.arch.sample_random_architecture(dataset_api=self.dataset_api)\n",
        "        # model.accuracy = model.arch.query(\n",
        "        #     self.performance_metric,\n",
        "        #     self.dataset,\n",
        "        #     epoch=self.fidelity,\n",
        "        #     dataset_api=self.dataset_api,\n",
        "        # )\n",
        "        # self.sampled_archs.append(model)\n",
        "        # self._update_history(model)\n",
        "\n",
        "        # All it does \n",
        "\n",
        "        ###########################################################\n",
        "        ##################### START TODO ##########################\n",
        "\n",
        "\n",
        "        # Write your logic here\n",
        "        # Also feel free to write new methods in this class\n",
        "\n",
        "\n",
        "        ##################### END TODO  ##########################\n",
        "        ##########################################################\n",
        "\n",
        "        self.sampled_archs.append(model) # This line is required. Add your chosen model to sampled_archs here.\n",
        "\n",
        "    def get_final_architecture(self):\n",
        "        \"\"\"\n",
        "        Returns the sampled architecture with the lowest validation error.\n",
        "        \"\"\"\n",
        "        return max(self.sampled_archs, key=lambda x: x.accuracy).arch\n",
        "\n",
        "    def train_statistics(self, report_incumbent: bool = True):\n",
        "\n",
        "        if report_incumbent:\n",
        "            best_arch = self.get_final_architecture()\n",
        "        else:\n",
        "            best_arch = self.sampled_archs[-1].arch\n",
        "\n",
        "        return (\n",
        "            best_arch.query(\n",
        "                Metric.TRAIN_ACCURACY, self.dataset, dataset_api=self.dataset_api\n",
        "            ),\n",
        "            best_arch.query(\n",
        "                Metric.VAL_ACCURACY, self.dataset, dataset_api=self.dataset_api\n",
        "            ),\n",
        "            best_arch.query(\n",
        "                Metric.TEST_ACCURACY, self.dataset, dataset_api=self.dataset_api\n",
        "            ),\n",
        "            best_arch.query(\n",
        "                Metric.TRAIN_TIME, self.dataset, dataset_api=self.dataset_api\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def test_statistics(self):\n",
        "        best_arch = self.get_final_architecture()\n",
        "        return best_arch.query(Metric.RAW, self.dataset, dataset_api=self.dataset_api)\n",
        "\n",
        "    def _update_history(self, child):\n",
        "        \"\"\"\n",
        "            We want to maintain only the history of the 100 best models in self.history\n",
        "            (self.sampled_archs stores the list of all models sampled.)\n",
        "        \"\"\"\n",
        "        if len(self.history) < 100:\n",
        "            self.history.append(child)\n",
        "        else:\n",
        "            for i, p in enumerate(self.history):\n",
        "                if child.accuracy > p.accuracy:\n",
        "                    self.history[i] = child\n",
        "                    break\n",
        "\n",
        "    def get_checkpointables(self):\n",
        "        return {\"models\": self.history}\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3a138bae05203fa8eb4bf07493da4bb9038fdb3e1f2f10b7ab3cd8c9223b9122"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d4a67abfa2e4eafa5a7c8cd1ce0c5c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e459c75405454a448dacf6fd02ac4fc6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_163f1d9f796d49a8931ce310de520917",
            "value": " 170498071/170498071 [00:03&lt;00:00, 46013119.91it/s]"
          }
        },
        "163f1d9f796d49a8931ce310de520917": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1794cb3d6bda42738ce9f9dfa01e9b5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bdc2c14d710d431b9095153276bb27df",
              "IPY_MODEL_98afd2dc6e9d4537b50d4ef04cf77a9f",
              "IPY_MODEL_0d4a67abfa2e4eafa5a7c8cd1ce0c5c0"
            ],
            "layout": "IPY_MODEL_aeea45fc34d04f62af72063e3fa83569"
          }
        },
        "84aa3f6831e2460896f5612df7f97170": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "98afd2dc6e9d4537b50d4ef04cf77a9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c22b9e474e154ad68f12a87b686a5b68",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84aa3f6831e2460896f5612df7f97170",
            "value": 170498071
          }
        },
        "aeea45fc34d04f62af72063e3fa83569": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b80c8d7c85de4e6d9e6476e6541d7e41": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdc2c14d710d431b9095153276bb27df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b80c8d7c85de4e6d9e6476e6541d7e41",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e9c9e94559394e1bbf170c5e01d8299a",
            "value": "100%"
          }
        },
        "c22b9e474e154ad68f12a87b686a5b68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e459c75405454a448dacf6fd02ac4fc6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9c9e94559394e1bbf170c5e01d8299a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
