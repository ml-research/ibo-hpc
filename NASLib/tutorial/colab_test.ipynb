{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uSb1Z9jCe2cU",
        "outputId": "ba9b6e09-a3de-4933-86a4-40335bc7750a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n",
            "Cloning into 'NASLib'...\n",
            "remote: Enumerating objects: 31761, done.\u001b[K\n",
            "remote: Counting objects: 100% (1717/1717), done.\u001b[K\n",
            "remote: Compressing objects: 100% (781/781), done.\u001b[K\n",
            "remote: Total 31761 (delta 1034), reused 1575 (delta 911), pack-reused 30044\u001b[K\n",
            "Receiving objects: 100% (31761/31761), 549.06 MiB | 16.59 MiB/s, done.\n",
            "Resolving deltas: 100% (22437/22437), done.\n",
            "/content/NASLib\n",
            "\u001b[33mWARNING: Skipping naslib as it is not installed.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 636 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.1 kB/s \n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 34.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 280 kB 57.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.0 MB 65.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166.7 MB 16 kB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 35.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 634 kB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 190 kB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 141 kB 68.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 75 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 44.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 187 kB 56.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 42.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 35.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 843 kB 51.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 42 kB 917 kB/s \n",
            "\u001b[K     |████████████████████████████████| 154 kB 60.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 960 kB 67.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 349 kB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 98 kB 7.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 66.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 468 kB 64.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 58.1 MB/s \n",
            "\u001b[?25h  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pybnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorwatch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for autograd-gamma (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pydotz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Error parsing requirements for lightgbm: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/lightgbm-2.2.3.dist-info/METADATA'\u001b[0m\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "thinc 8.1.0 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\n",
            "spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.3.0 which is incompatible.\u001b[0m\n",
            "Found existing installation: matplotlib 3.5.1\n",
            "Uninstalling matplotlib-3.5.1:\n",
            "  Successfully uninstalled matplotlib-3.5.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting matplotlib==2.1.1\n",
            "  Downloading matplotlib-2.1.1.tar.gz (36.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 36.1 MB 227 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (1.21.5)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (2022.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==2.1.1) (3.0.9)\n",
            "Building wheels for collected packages: matplotlib\n",
            "  Building wheel for matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for matplotlib: filename=matplotlib-2.1.1-cp37-cp37m-linux_x86_64.whl size=10243719 sha256=bcbe30a0c32801b48eed448d1af56a5d8ffbf5b9e0b98000dbd1d7f5d1d7e867\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/7a/d6/021782cff10d8257e030d4a766ca5ed9667fd8758606fbbeff\n",
            "Successfully built matplotlib\n",
            "Installing collected packages: matplotlib\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "seaborn 0.11.2 requires matplotlib>=2.2, but you have matplotlib 2.1.1 which is incompatible.\n",
            "plotnine 0.8.0 requires matplotlib>=3.1.1, but you have matplotlib 2.1.1 which is incompatible.\n",
            "mizani 0.7.3 requires matplotlib>=3.1.1, but you have matplotlib 2.1.1 which is incompatible.\n",
            "lifelines 0.27.3 requires matplotlib>=3.0, but you have matplotlib 2.1.1 which is incompatible.\n",
            "datascience 0.17.5 requires matplotlib>=3.0.0, but you have matplotlib 2.1.1 which is incompatible.\n",
            "arviz 0.12.1 requires matplotlib>=3.0, but you have matplotlib 2.1.1 which is incompatible.\n",
            "naslib 0.1.0 requires lightgbm==3.2.1, but you have lightgbm 3.3.2 which is incompatible.\n",
            "naslib 0.1.0 requires matplotlib==3.5.1, but you have matplotlib 2.1.1 which is incompatible.\u001b[0m\n",
            "Successfully installed matplotlib-2.1.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Rhea's version\n",
        "%cd /content\n",
        "%rm -rf NASLib\n",
        "!git clone -b Develop_copy https://github.com/automl/NASLib/\n",
        "%cd /content/NASLib\n",
        "!pip install -e . --quiet\n",
        "!pip uninstall --yes matplotlib\n",
        "!pip install matplotlib==2.1.1\n",
        "exit()\n",
        "# The runtime would be automatically restarted now. Just proceed with running the next cell now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6N6YXVNgDgE",
        "outputId": "e1ec2135-2ec8-4865-d1b6-9751c8204cd2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/skimage/io/manage_plugins.py:23: UserWarning: Your installed pillow version is < 8.1.2. Several security issues (CVE-2021-27921, CVE-2021-25290, CVE-2021-25291, CVE-2021-25293, and more) have been fixed in pillow 8.1.2 or higher. We recommend to upgrade this library.\n",
            "  from .collection import imread_collection_wrapper\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0, 4, 1, 0, 4, 4)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import the search space\n",
        "from naslib.search_spaces import NasBench201SearchSpace\n",
        "# Sample a random architecture\n",
        "# You can call this method only once.\n",
        "graph = NasBench201SearchSpace(n_classes=10)\n",
        "graph.sample_random_architecture()\n",
        "\n",
        "# Get the NASLib representation of this architecture\n",
        "graph.get_hash()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJTiGHQGgVGj",
        "outputId": "80723dbc-4e65-4507-f6bc-91556861d846"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:naslib.search_spaces.core.graph:Comb_op is ignored if subgraph is defined!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([5, 10])\n"
          ]
        }
      ],
      "source": [
        "## This graph is now a NAS-Bench-201 model, which can be used for training\n",
        "# Forward pass some dummy data through it to see it in action\n",
        "\n",
        "import torch\n",
        "\n",
        "x = torch.randn(5, 3, 32, 32) # (Batch_size, Num_channels, Height, Width)\n",
        "logits = graph(x)\n",
        "\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xiBRlprFgu-f",
        "outputId": "6b2a3d47-152d-4714-e2c3-3129ae34e474"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'|skip_connect~0|+|avg_pool_3x3~0|skip_connect~1|+|none~0|avg_pool_3x3~1|avg_pool_3x3~2|'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import code to convert NASLib graph to the original NAS-Bench-201 representation\n",
        "from naslib.search_spaces.nasbench201.conversions import convert_naslib_to_str as convert_naslib_nb201_to_str\n",
        "\n",
        "# Get the string representation of this model\n",
        "convert_naslib_nb201_to_str(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW7DE81fg129",
        "outputId": "4955292d-88ef-483a-9266-4bdeb8ffef98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parent graph: (0, 4, 1, 0, 4, 4)\n",
            "Child graph : (0, 4, 1, 0, 3, 4)\n"
          ]
        }
      ],
      "source": [
        "# Mutating an architecture\n",
        "# First, create a new child_graph\n",
        "child_graph = NasBench201SearchSpace(n_classes=10)\n",
        "\n",
        "# Call mutate on the child graph by passing the parent graph to it\n",
        "child_graph.mutate(parent=graph)\n",
        "\n",
        "# See the parent and child graph representations\n",
        "print(f'Parent graph: {graph.get_hash()}')\n",
        "print(f'Child graph : {child_graph.get_hash()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6MekX0Gg9vB",
        "outputId": "d782122c-2a7c-40c0-f5eb-8329a2e07121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/NASLib\n",
            "dataset = cifar10\n",
            "search_space = nb201\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sh8pEhdrgZ97-VFBVL94rI36gedExVgJ\n",
            "To: /content/NASLib/naslib/data/nb201_cifar10_full_training.pickle\n",
            "100% 117M/117M [00:00<00:00, 178MB/s]\n"
          ]
        }
      ],
      "source": [
        "%cd NASLib\n",
        "!source /content/NASLib/scripts/bash_scripts/download_benchmarks.sh nb201 cifar10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BC0WXCu1g5o6"
      },
      "outputs": [],
      "source": [
        "# Now, let's load the queryable tabular NAS-Bench-201 API\n",
        "# This API has the training metrics of all the 15625 models in the search space\n",
        "# such as train and validation accuracies/losses at every epoch\n",
        "\n",
        "from naslib.utils import get_dataset_api\n",
        "benchmark_api = get_dataset_api(search_space='nasbench201', dataset='cifar10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZauaq6Mg8R0",
        "outputId": "441f8e61-10f5-4097-b913-64319f42c90d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance of parent model\n",
            "Train accuracy: 55.53600000366211\n",
            "Validation accuracy: 53.99\n"
          ]
        }
      ],
      "source": [
        "# Now, let's load the queryable tabular NAS-Bench-201 API\n",
        "# This API has the training metrics of all the 15625 models in the search space\n",
        "# such as train and validation accuracies/losses at every epoch\n",
        "\n",
        "from naslib.utils import get_dataset_api\n",
        "benchmark_api = get_dataset_api(search_space='nasbench201', dataset='cifar10')\n",
        "# With the NAS-Bench-201 API, we can now query, say, the validation performance of any NB201 model\n",
        "# Without it, we would have to train the model from scratch to get this information\n",
        "\n",
        "# First, import the Metric enum\n",
        "from naslib.search_spaces.core import Metric\n",
        "\n",
        "# Metric has, among others, these values:\n",
        "# Metric.TRAIN_ACCURACY\n",
        "# Metric.VAL_ACCURACY\n",
        "# Metric.TRAIN_LOSS\n",
        "# Metric.TEST_LOSS\n",
        "# Metric.TRAIN_TIME\n",
        "\n",
        "train_acc_parent = graph.query(metric=Metric.TRAIN_ACCURACY, dataset='cifar10', dataset_api=benchmark_api)\n",
        "val_acc_parent = graph.query(metric=Metric.VAL_ACCURACY, dataset='cifar10', dataset_api=benchmark_api)\n",
        "\n",
        "print('Performance of parent model')\n",
        "print(f'Train accuracy: {train_acc_parent}')\n",
        "print(f'Validation accuracy: {val_acc_parent}')\n",
        "\n",
        "# TODO: Query the train and validation performance of the child model\n",
        "# train_acc_parent = ...\n",
        "# val_acc_parent = ...\n",
        "\n",
        "# print('Performance of child model')\n",
        "# print(f'Train accuracy: {train_acc_child}')\n",
        "# print(f'Validation accuracy: {val_acc_child}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cPBKXh6ihotW"
      },
      "outputs": [],
      "source": [
        "from naslib.search_spaces import NasBench301SearchSpace\n",
        "from naslib.search_spaces.nasbench301.conversions import convert_naslib_to_genotype as convert_naslib_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "acdJoxNVhtWx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# import the Trainer used to run the optimizer on a given search space\n",
        "from naslib.defaults.trainer import Trainer\n",
        "# import the optimizers\n",
        "from naslib.optimizers import (\n",
        "    RandomSearch,\n",
        "    RegularizedEvolution\n",
        ")\n",
        "# import the search spaces\n",
        "from naslib.search_spaces import (\n",
        "    NasBench101SearchSpace,\n",
        "    NasBench201SearchSpace,\n",
        "    NasBench301SearchSpace,\n",
        ")\n",
        "\n",
        "from naslib.search_spaces.core.query_metrics import Metric\n",
        "from naslib import utils\n",
        "from naslib.utils import get_dataset_api\n",
        "from naslib.utils.log import setup_logger\n",
        "\n",
        "from fvcore.common.config import CfgNode # Required to read the config\n",
        "###### End of imports ######\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewlTAIyEh2U1",
        "outputId": "5df07657-6982-4039-8de8-bc69ccaa7df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/06 13:41:31 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs\n",
            "search:\n",
            "  checkpoint_freq: 100\n",
            "  epochs: 5\n",
            "  fidelity: -1\n",
            "  seed: 0\n"
          ]
        }
      ],
      "source": [
        "# The configuration used by the Trainer and Optimizer\n",
        "config = {\n",
        "    'dataset': 'cifar10',\n",
        "    'search': {\n",
        "        'seed': 0, # \n",
        "        'epochs': 5, # Number of epochs (steps) of the optimizer to run\n",
        "        'fidelity': -1, # \n",
        "        'checkpoint_freq': 100,\n",
        "    },\n",
        "    'save': 'runs' # folder to save the results to \n",
        "}\n",
        "\n",
        "config = CfgNode.load_cfg(json.dumps(config))\n",
        "\n",
        "# Make the directories required for search and evaluation\n",
        "os.makedirs(config['save'] + '/search', exist_ok=True)\n",
        "os.makedirs(config['save'] + '/eval', exist_ok=True)\n",
        "\n",
        "# Set up the loggers\n",
        "logger = setup_logger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# See the config\n",
        "logger.info(f'Configuration is \\n{config}')\n",
        "# logger.info(config)\n",
        "\n",
        "# Set up the seeds\n",
        "utils.set_seed(9002)\n",
        "\n",
        "# Instantiate the search space and get its bench"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKphfcOmh55-",
        "outputId": "e3471e78-7ecf-4be4-c38d-d3d35bcdfbf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/06 13:41:47 nl.defaults.trainer]: \u001b[0mparam size = 0.000000MB\n",
            "\u001b[32m[10/06 13:41:47 nl.defaults.trainer]: \u001b[0mStart training\n",
            "\u001b[32m[10/06 13:41:47 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 99.11600, Validation accuracy: 86.81000\n",
            "\u001b[32m[10/06 13:41:47 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 99.55200, Validation accuracy: 84.74000\n",
            "\u001b[32m[10/06 13:41:47 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 90.96800, Validation accuracy: 84.01000\n",
            "\u001b[32m[10/06 13:41:49 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 99.16800, Validation accuracy: 84.36000\n",
            "\u001b[32m[10/06 13:41:49 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 99.96400, Validation accuracy: 90.21000\n",
            "\u001b[32m[10/06 13:41:49 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [99.116, 99.5520000024414, 90.96799999023438, 99.16800000732422, 99.964]\n",
            "Validation accuracies: [86.81, 84.74, 84.01, 84.36, 90.21]\n",
            "\u001b[32m[10/06 13:41:49 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/06 13:41:49 nl.defaults.trainer]: \u001b[0mloading model from file runs/search/model_final.pth\n",
            "\u001b[32m[10/06 13:41:49 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (2, 3, 0, 1, 3, 3)\n",
            "\u001b[32m[10/06 13:41:49 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 90.21\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "90.21"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "search_space = NasBench201SearchSpace()\n",
        "dataset_api = get_dataset_api('nasbench201', 'cifar10')\n",
        "\n",
        "# Instantitate the optimizer and adapt the search space to it\n",
        "optimizer = RandomSearch(config)\n",
        "optimizer.adapt_search_space(search_space, dataset_api=dataset_api)\n",
        "\n",
        "# Create a Trainer\n",
        "trainer = Trainer(optimizer, config)\n",
        "\n",
        "# Perform the search\n",
        "trainer.search(resume_from=\"\", report_incumbent=False)\n",
        "\n",
        "# Get the results of the search\n",
        "search_trajectory = trainer.search_trajectory\n",
        "print('Train accuracies:', search_trajectory.train_acc)\n",
        "print('Validation accuracies:', search_trajectory.valid_acc)\n",
        "\n",
        "# Get the validation performance of the best model found in the search phase\n",
        "best_model_val_acc = trainer.evaluate(dataset_api=dataset_api, metric=Metric.VAL_ACCURACY)\n",
        "best_model_val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "9aW2rKQth9YR"
      },
      "outputs": [],
      "source": [
        "def update_config(config, optimizer_type, search_space_type, dataset, seed):\n",
        "    # Dataset being used\n",
        "    config.dataset = dataset\n",
        "    \n",
        "    # Directory to which the results/logs will be saved\n",
        "    config.save = f\"runs/{optimizer_type.__name__}/{search_space_type.__name__}/{dataset}/{seed}\"\n",
        "    \n",
        "    # Seed used during search phase of the optimizer\n",
        "    config.search.seed = seed\n",
        "    \n",
        "def run_optimizer(optimizer_type, search_space_type, dataset, dataset_api, config, seed):\n",
        "    # Update the config\n",
        "    update_config(config, optimizer_type, search_space_type, dataset, seed)\n",
        "\n",
        "    # Make the results directories\n",
        "    os.makedirs(config.save + '/search', exist_ok=True)\n",
        "    os.makedirs(config.save + '/eval', exist_ok=True)\n",
        "\n",
        "    # Set up the loggers\n",
        "    logger = setup_logger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "     # See the config\n",
        "    logger.info(f'Configuration is \\n{config}')\n",
        "\n",
        "    # Set up the seed\n",
        "    utils.set_seed(seed)\n",
        "\n",
        "    # Instantiate the search space\n",
        "    n_classes = {\n",
        "        'cifar10': 10,\n",
        "        'cifar100': 100,\n",
        "        'ImageNet16-120': 120\n",
        "    }\n",
        "    search_space = search_space_type(n_classes=n_classes[dataset])\n",
        "\n",
        "    # Get the benchmark API\n",
        "    logger.info('Loading Benchmark API')\n",
        "    dataset_api = get_dataset_api(search_space.get_type(), dataset)\n",
        "    \n",
        "    # Instantiate the optimizer and adapat the search space to the optimizer\n",
        "    optimizer = optimizer_type(config)\n",
        "    optimizer.adapt_search_space(search_space, dataset_api=dataset_api)\n",
        "\n",
        "    # Create a Trainer\n",
        "    trainer = Trainer(optimizer, config)\n",
        "\n",
        "    # Perform the search\n",
        "    trainer.search(report_incumbent=False)\n",
        "    # Get the results of the search\n",
        "    search_trajectory = trainer.search_trajectory\n",
        "    print('Train accuracies:', search_trajectory.train_acc)\n",
        "    print('Validation accuracies:', search_trajectory.valid_acc)\n",
        "\n",
        "    # Get the validation performance of the best model found in the search phase\n",
        "    best_model_val_acc = trainer.evaluate(dataset_api=dataset_api, metric=Metric.VAL_ACCURACY)\n",
        "    best_model_val_acc\n",
        "\n",
        "    best_model = optimizer.get_final_architecture()\n",
        "\n",
        "    return search_trajectory, best_model, best_model_val_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pbx_jyaiJ_Q",
        "outputId": "9342e3a7-aee5-4003-9f2c-671bef61df2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m[10/06 13:43:15 naslib]: \u001b[0mConfiguration is \n",
            "dataset: cifar10\n",
            "save: runs/RegularizedEvolution/NasBench201SearchSpace/cifar10/9001\n",
            "search:\n",
            "  checkpoint_freq: 100\n",
            "  epochs: 100\n",
            "  fidelity: -1\n",
            "  population_size: 30\n",
            "  sample_size: 10\n",
            "  seed: 9001\n",
            "\u001b[32m[10/06 13:43:15 naslib]: \u001b[0mLoading Benchmark API\n",
            "\u001b[32m[10/06 13:43:19 nl.defaults.trainer]: \u001b[0mparam size = 0.000000MB\n",
            "\u001b[32m[10/06 13:43:19 nl.defaults.trainer]: \u001b[0mStart training\n",
            "\u001b[32m[10/06 13:43:19 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:19 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 1\n",
            "\u001b[32m[10/06 13:43:19 nl.defaults.trainer]: \u001b[0mEpoch 0 done. Train accuracy: 79.06800, Validation accuracy: 74.56000\n",
            "\u001b[32m[10/06 13:43:19 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:19 nl.defaults.trainer]: \u001b[0mEpoch 1 done. Train accuracy: 97.60000, Validation accuracy: 85.57000\n",
            "\u001b[32m[10/06 13:43:19 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:19 nl.defaults.trainer]: \u001b[0mEpoch 2 done. Train accuracy: 99.92800, Validation accuracy: 88.93000\n",
            "\u001b[32m[10/06 13:43:19 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:19 nl.defaults.trainer]: \u001b[0mEpoch 3 done. Train accuracy: 81.00400, Validation accuracy: 70.60000\n",
            "\u001b[32m[10/06 13:43:19 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:20 nl.defaults.trainer]: \u001b[0mEpoch 4 done. Train accuracy: 99.79200, Validation accuracy: 85.41000\n",
            "\u001b[32m[10/06 13:43:20 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:20 nl.defaults.trainer]: \u001b[0mEpoch 5 done. Train accuracy: 99.93200, Validation accuracy: 88.89000\n",
            "\u001b[32m[10/06 13:43:20 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:20 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 7\n",
            "\u001b[32m[10/06 13:43:20 nl.defaults.trainer]: \u001b[0mEpoch 6 done. Train accuracy: 99.95600, Validation accuracy: 88.50000\n",
            "\u001b[32m[10/06 13:43:20 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:20 nl.defaults.trainer]: \u001b[0mEpoch 7 done. Train accuracy: 97.50000, Validation accuracy: 86.11000\n",
            "\u001b[32m[10/06 13:43:20 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:21 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 9\n",
            "\u001b[32m[10/06 13:43:21 nl.defaults.trainer]: \u001b[0mEpoch 8 done. Train accuracy: 90.93200, Validation accuracy: 84.08000\n",
            "\u001b[32m[10/06 13:43:21 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:22 nl.defaults.trainer]: \u001b[0mEpoch 9 done. Train accuracy: 96.22800, Validation accuracy: 84.21000\n",
            "\u001b[32m[10/06 13:43:22 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:22 nl.defaults.trainer]: \u001b[0mEpoch 10 done. Train accuracy: 81.25600, Validation accuracy: 71.96000\n",
            "\u001b[32m[10/06 13:43:22 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:22 nl.defaults.trainer]: \u001b[0mEpoch 11 done. Train accuracy: 99.88000, Validation accuracy: 88.23000\n",
            "\u001b[32m[10/06 13:43:22 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:22 nl.defaults.trainer]: \u001b[0mEpoch 12 done. Train accuracy: 99.98800, Validation accuracy: 90.25000\n",
            "\u001b[32m[10/06 13:43:22 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:22 nl.defaults.trainer]: \u001b[0mEpoch 13 done. Train accuracy: 96.14400, Validation accuracy: 85.36000\n",
            "\u001b[32m[10/06 13:43:22 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:23 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 15\n",
            "\u001b[32m[10/06 13:43:23 nl.defaults.trainer]: \u001b[0mEpoch 14 done. Train accuracy: 86.77200, Validation accuracy: 78.78000\n",
            "\u001b[32m[10/06 13:43:23 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:23 nl.defaults.trainer]: \u001b[0mEpoch 15 done. Train accuracy: 99.45600, Validation accuracy: 85.47000\n",
            "\u001b[32m[10/06 13:43:23 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:23 nl.defaults.trainer]: \u001b[0mEpoch 16 done. Train accuracy: 97.48800, Validation accuracy: 78.99000\n",
            "\u001b[32m[10/06 13:43:23 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:23 nl.defaults.trainer]: \u001b[0mEpoch 17 done. Train accuracy: 92.13200, Validation accuracy: 83.30000\n",
            "\u001b[32m[10/06 13:43:23 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:23 nl.defaults.trainer]: \u001b[0mEpoch 18 done. Train accuracy: 99.96000, Validation accuracy: 89.24000\n",
            "\u001b[32m[10/06 13:43:23 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:25 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 20\n",
            "\u001b[32m[10/06 13:43:25 nl.defaults.trainer]: \u001b[0mEpoch 19 done. Train accuracy: 99.89600, Validation accuracy: 87.51000\n",
            "\u001b[32m[10/06 13:43:25 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:25 nl.defaults.trainer]: \u001b[0mEpoch 20 done. Train accuracy: 81.10000, Validation accuracy: 71.41000\n",
            "\u001b[32m[10/06 13:43:25 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:25 nl.defaults.trainer]: \u001b[0mEpoch 21 done. Train accuracy: 99.89200, Validation accuracy: 87.75000\n",
            "\u001b[32m[10/06 13:43:25 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:25 nl.defaults.trainer]: \u001b[0mEpoch 22 done. Train accuracy: 97.62400, Validation accuracy: 85.98000\n",
            "\u001b[32m[10/06 13:43:25 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:26 nl.defaults.trainer]: \u001b[0mEpoch 23 done. Train accuracy: 99.79200, Validation accuracy: 87.97000\n",
            "\u001b[32m[10/06 13:43:26 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:26 nl.defaults.trainer]: \u001b[0mEpoch 24 done. Train accuracy: 99.23600, Validation accuracy: 86.52000\n",
            "\u001b[32m[10/06 13:43:26 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:26 nl.optimizers.discrete.re.optimizer]: \u001b[0mPopulation size 26\n",
            "\u001b[32m[10/06 13:43:26 nl.defaults.trainer]: \u001b[0mEpoch 25 done. Train accuracy: 87.23600, Validation accuracy: 80.38000\n",
            "\u001b[32m[10/06 13:43:26 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:26 nl.defaults.trainer]: \u001b[0mEpoch 26 done. Train accuracy: 99.88400, Validation accuracy: 89.06000\n",
            "\u001b[32m[10/06 13:43:26 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:26 nl.defaults.trainer]: \u001b[0mEpoch 27 done. Train accuracy: 99.86800, Validation accuracy: 88.43000\n",
            "\u001b[32m[10/06 13:43:26 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:27 nl.defaults.trainer]: \u001b[0mEpoch 28 done. Train accuracy: 96.81200, Validation accuracy: 85.09000\n",
            "\u001b[32m[10/06 13:43:27 nl.optimizers.discrete.re.optimizer]: \u001b[0mStart sampling architectures to fill the population\n",
            "\u001b[32m[10/06 13:43:27 nl.defaults.trainer]: \u001b[0mEpoch 29 done. Train accuracy: 33.21600, Validation accuracy: 33.93000\n",
            "\u001b[32m[10/06 13:43:27 nl.defaults.trainer]: \u001b[0mEpoch 30 done. Train accuracy: 99.77200, Validation accuracy: 87.73000\n",
            "\u001b[32m[10/06 13:43:27 nl.defaults.trainer]: \u001b[0mEpoch 31 done. Train accuracy: 99.96400, Validation accuracy: 90.29000\n",
            "\u001b[32m[10/06 13:43:29 nl.defaults.trainer]: \u001b[0mEpoch 32 done. Train accuracy: 99.92800, Validation accuracy: 88.62000\n",
            "\u001b[32m[10/06 13:43:29 nl.defaults.trainer]: \u001b[0mEpoch 33 done. Train accuracy: 98.12000, Validation accuracy: 86.57000\n",
            "\u001b[32m[10/06 13:43:29 nl.defaults.trainer]: \u001b[0mEpoch 34 done. Train accuracy: 99.89600, Validation accuracy: 89.02000\n",
            "\u001b[32m[10/06 13:43:29 nl.defaults.trainer]: \u001b[0mEpoch 35 done. Train accuracy: 99.97200, Validation accuracy: 89.79000\n",
            "\u001b[32m[10/06 13:43:30 nl.defaults.trainer]: \u001b[0mEpoch 36 done. Train accuracy: 99.97200, Validation accuracy: 89.79000\n",
            "\u001b[32m[10/06 13:43:30 nl.defaults.trainer]: \u001b[0mEpoch 37 done. Train accuracy: 99.98000, Validation accuracy: 88.96000\n",
            "\u001b[32m[10/06 13:43:30 nl.defaults.trainer]: \u001b[0mEpoch 38 done. Train accuracy: 99.68800, Validation accuracy: 87.03000\n",
            "\u001b[32m[10/06 13:43:30 nl.defaults.trainer]: \u001b[0mEpoch 39 done. Train accuracy: 99.58800, Validation accuracy: 86.90000\n",
            "\u001b[32m[10/06 13:43:30 nl.defaults.trainer]: \u001b[0mEpoch 40 done. Train accuracy: 99.98800, Validation accuracy: 88.97000\n",
            "\u001b[32m[10/06 13:43:30 nl.defaults.trainer]: \u001b[0mEpoch 41 done. Train accuracy: 99.97200, Validation accuracy: 89.79000\n",
            "\u001b[32m[10/06 13:43:31 nl.defaults.trainer]: \u001b[0mEpoch 42 done. Train accuracy: 99.99200, Validation accuracy: 90.74000\n",
            "\u001b[32m[10/06 13:43:31 nl.defaults.trainer]: \u001b[0mEpoch 43 done. Train accuracy: 99.97200, Validation accuracy: 89.89000\n",
            "\u001b[32m[10/06 13:43:31 nl.defaults.trainer]: \u001b[0mEpoch 44 done. Train accuracy: 99.96800, Validation accuracy: 89.81000\n",
            "\u001b[32m[10/06 13:43:31 nl.defaults.trainer]: \u001b[0mEpoch 45 done. Train accuracy: 99.98800, Validation accuracy: 89.75000\n",
            "\u001b[32m[10/06 13:43:33 nl.defaults.trainer]: \u001b[0mEpoch 46 done. Train accuracy: 99.97600, Validation accuracy: 89.92000\n",
            "\u001b[32m[10/06 13:43:34 nl.defaults.trainer]: \u001b[0mEpoch 47 done. Train accuracy: 99.97600, Validation accuracy: 89.79000\n",
            "\u001b[32m[10/06 13:43:34 nl.defaults.trainer]: \u001b[0mEpoch 48 done. Train accuracy: 99.99600, Validation accuracy: 89.93000\n",
            "\u001b[32m[10/06 13:43:34 nl.defaults.trainer]: \u001b[0mEpoch 49 done. Train accuracy: 99.95200, Validation accuracy: 89.84000\n",
            "\u001b[32m[10/06 13:43:35 nl.defaults.trainer]: \u001b[0mEpoch 50 done. Train accuracy: 99.91600, Validation accuracy: 88.76000\n",
            "\u001b[32m[10/06 13:43:35 nl.defaults.trainer]: \u001b[0mEpoch 51 done. Train accuracy: 99.96400, Validation accuracy: 90.07000\n",
            "\u001b[32m[10/06 13:43:35 nl.defaults.trainer]: \u001b[0mEpoch 52 done. Train accuracy: 99.97200, Validation accuracy: 89.99000\n",
            "\u001b[32m[10/06 13:43:36 nl.defaults.trainer]: \u001b[0mEpoch 53 done. Train accuracy: 99.96800, Validation accuracy: 89.81000\n",
            "\u001b[32m[10/06 13:43:36 nl.defaults.trainer]: \u001b[0mEpoch 54 done. Train accuracy: 99.97600, Validation accuracy: 90.36000\n",
            "\u001b[32m[10/06 13:43:37 nl.defaults.trainer]: \u001b[0mEpoch 55 done. Train accuracy: 99.98800, Validation accuracy: 89.75000\n",
            "\u001b[32m[10/06 13:43:37 nl.defaults.trainer]: \u001b[0mEpoch 56 done. Train accuracy: 99.97200, Validation accuracy: 89.79000\n",
            "\u001b[32m[10/06 13:43:37 nl.defaults.trainer]: \u001b[0mEpoch 57 done. Train accuracy: 99.98000, Validation accuracy: 89.79000\n",
            "\u001b[32m[10/06 13:43:38 nl.defaults.trainer]: \u001b[0mEpoch 58 done. Train accuracy: 99.96000, Validation accuracy: 90.60000\n",
            "\u001b[32m[10/06 13:43:38 nl.defaults.trainer]: \u001b[0mEpoch 59 done. Train accuracy: 99.98000, Validation accuracy: 89.95000\n",
            "\u001b[32m[10/06 13:43:38 nl.defaults.trainer]: \u001b[0mEpoch 60 done. Train accuracy: 99.94800, Validation accuracy: 89.41000\n",
            "\u001b[32m[10/06 13:43:39 nl.defaults.trainer]: \u001b[0mEpoch 61 done. Train accuracy: 99.94000, Validation accuracy: 89.64000\n",
            "\u001b[32m[10/06 13:43:39 nl.defaults.trainer]: \u001b[0mEpoch 62 done. Train accuracy: 99.97600, Validation accuracy: 90.36000\n",
            "\u001b[32m[10/06 13:43:41 nl.defaults.trainer]: \u001b[0mEpoch 63 done. Train accuracy: 99.92400, Validation accuracy: 88.88000\n",
            "\u001b[32m[10/06 13:43:41 nl.defaults.trainer]: \u001b[0mEpoch 64 done. Train accuracy: 99.96800, Validation accuracy: 90.58000\n",
            "\u001b[32m[10/06 13:43:41 nl.defaults.trainer]: \u001b[0mEpoch 65 done. Train accuracy: 99.93600, Validation accuracy: 89.81000\n",
            "\u001b[32m[10/06 13:43:42 nl.defaults.trainer]: \u001b[0mEpoch 66 done. Train accuracy: 99.98800, Validation accuracy: 90.33000\n",
            "\u001b[32m[10/06 13:43:42 nl.defaults.trainer]: \u001b[0mEpoch 67 done. Train accuracy: 95.91600, Validation accuracy: 85.94000\n",
            "\u001b[32m[10/06 13:43:42 nl.defaults.trainer]: \u001b[0mEpoch 68 done. Train accuracy: 99.97600, Validation accuracy: 90.28000\n",
            "\u001b[32m[10/06 13:43:42 nl.defaults.trainer]: \u001b[0mEpoch 69 done. Train accuracy: 99.99200, Validation accuracy: 90.20000\n",
            "\u001b[32m[10/06 13:43:42 nl.defaults.trainer]: \u001b[0mEpoch 70 done. Train accuracy: 99.93200, Validation accuracy: 89.58000\n",
            "\u001b[32m[10/06 13:43:43 nl.defaults.trainer]: \u001b[0mEpoch 71 done. Train accuracy: 99.95600, Validation accuracy: 89.40000\n",
            "\u001b[32m[10/06 13:43:43 nl.defaults.trainer]: \u001b[0mEpoch 72 done. Train accuracy: 99.98000, Validation accuracy: 89.79000\n",
            "\u001b[32m[10/06 13:43:43 nl.defaults.trainer]: \u001b[0mEpoch 73 done. Train accuracy: 99.99200, Validation accuracy: 89.76000\n",
            "\u001b[32m[10/06 13:43:43 nl.defaults.trainer]: \u001b[0mEpoch 74 done. Train accuracy: 99.95600, Validation accuracy: 90.40000\n",
            "\u001b[32m[10/06 13:43:43 nl.defaults.trainer]: \u001b[0mEpoch 75 done. Train accuracy: 99.98800, Validation accuracy: 90.98000\n",
            "\u001b[32m[10/06 13:43:44 nl.defaults.trainer]: \u001b[0mEpoch 76 done. Train accuracy: 99.96800, Validation accuracy: 90.25000\n",
            "\u001b[32m[10/06 13:43:44 nl.defaults.trainer]: \u001b[0mEpoch 77 done. Train accuracy: 99.95600, Validation accuracy: 90.40000\n",
            "\u001b[32m[10/06 13:43:44 nl.defaults.trainer]: \u001b[0mEpoch 78 done. Train accuracy: 99.96800, Validation accuracy: 90.65000\n",
            "\u001b[32m[10/06 13:43:44 nl.defaults.trainer]: \u001b[0mEpoch 79 done. Train accuracy: 99.99600, Validation accuracy: 90.40000\n",
            "\u001b[32m[10/06 13:43:44 nl.defaults.trainer]: \u001b[0mEpoch 80 done. Train accuracy: 99.96400, Validation accuracy: 90.29000\n",
            "\u001b[32m[10/06 13:43:45 nl.defaults.trainer]: \u001b[0mEpoch 81 done. Train accuracy: 99.98800, Validation accuracy: 90.67000\n",
            "\u001b[32m[10/06 13:43:45 nl.defaults.trainer]: \u001b[0mEpoch 82 done. Train accuracy: 99.93600, Validation accuracy: 88.43000\n",
            "\u001b[32m[10/06 13:43:47 nl.defaults.trainer]: \u001b[0mEpoch 83 done. Train accuracy: 99.96400, Validation accuracy: 90.75000\n",
            "\u001b[32m[10/06 13:43:47 nl.defaults.trainer]: \u001b[0mEpoch 84 done. Train accuracy: 99.95200, Validation accuracy: 88.89000\n",
            "\u001b[32m[10/06 13:43:47 nl.defaults.trainer]: \u001b[0mEpoch 85 done. Train accuracy: 98.12000, Validation accuracy: 86.72000\n",
            "\u001b[32m[10/06 13:43:47 nl.defaults.trainer]: \u001b[0mEpoch 86 done. Train accuracy: 99.94400, Validation accuracy: 89.66000\n",
            "\u001b[32m[10/06 13:43:47 nl.defaults.trainer]: \u001b[0mEpoch 87 done. Train accuracy: 99.98400, Validation accuracy: 90.67000\n",
            "\u001b[32m[10/06 13:43:47 nl.defaults.trainer]: \u001b[0mEpoch 88 done. Train accuracy: 99.98400, Validation accuracy: 89.86000\n",
            "\u001b[32m[10/06 13:43:48 nl.defaults.trainer]: \u001b[0mEpoch 89 done. Train accuracy: 99.98400, Validation accuracy: 89.86000\n",
            "\u001b[32m[10/06 13:43:48 nl.defaults.trainer]: \u001b[0mEpoch 90 done. Train accuracy: 99.98000, Validation accuracy: 90.51000\n",
            "\u001b[32m[10/06 13:43:48 nl.defaults.trainer]: \u001b[0mEpoch 91 done. Train accuracy: 99.97200, Validation accuracy: 90.15000\n",
            "\u001b[32m[10/06 13:43:48 nl.defaults.trainer]: \u001b[0mEpoch 92 done. Train accuracy: 99.42400, Validation accuracy: 86.75000\n",
            "\u001b[32m[10/06 13:43:48 nl.defaults.trainer]: \u001b[0mEpoch 93 done. Train accuracy: 99.97200, Validation accuracy: 90.15000\n",
            "\u001b[32m[10/06 13:43:49 nl.defaults.trainer]: \u001b[0mEpoch 94 done. Train accuracy: 99.96400, Validation accuracy: 88.45000\n",
            "\u001b[32m[10/06 13:43:49 nl.defaults.trainer]: \u001b[0mEpoch 95 done. Train accuracy: 99.97200, Validation accuracy: 89.29000\n",
            "\u001b[32m[10/06 13:43:49 nl.defaults.trainer]: \u001b[0mEpoch 96 done. Train accuracy: 99.98000, Validation accuracy: 90.51000\n",
            "\u001b[32m[10/06 13:43:49 nl.defaults.trainer]: \u001b[0mEpoch 97 done. Train accuracy: 99.96400, Validation accuracy: 88.45000\n",
            "\u001b[32m[10/06 13:43:49 nl.defaults.trainer]: \u001b[0mEpoch 98 done. Train accuracy: 99.98000, Validation accuracy: 90.65000\n",
            "\u001b[32m[10/06 13:43:50 nl.defaults.trainer]: \u001b[0mEpoch 99 done. Train accuracy: 91.00800, Validation accuracy: 84.22000\n",
            "\u001b[32m[10/06 13:43:50 nl.defaults.trainer]: \u001b[0mTraining finished\n",
            "Train accuracies: [79.06799998779297, 97.60000000976562, 99.928, 81.00399999511718, 99.792, 99.93200000244141, 99.956, 97.50000000732422, 90.93200000488281, 96.22800001953125, 81.25599998535156, 99.88, 99.988, 96.14400001708984, 86.772, 99.45600000488281, 97.48800001220702, 92.13199998779297, 99.96, 99.896, 81.09999998535156, 99.8920000024414, 97.62399997070312, 99.79200000244141, 99.2360000024414, 87.23600000976562, 99.884, 99.868, 96.81199997070313, 33.21600000976562, 99.772, 99.964, 99.928, 98.1200000024414, 99.896, 99.972, 99.972, 99.98, 99.688, 99.5880000024414, 99.988, 99.972, 99.992, 99.972, 99.968, 99.988, 99.976, 99.976, 99.996, 99.952, 99.916, 99.964, 99.972, 99.968, 99.976, 99.988, 99.972, 99.98, 99.96, 99.98, 99.948, 99.94000000244141, 99.976, 99.9240000024414, 99.968, 99.936, 99.988, 95.91600000732421, 99.976, 99.992, 99.932, 99.956, 99.98, 99.992, 99.956, 99.988, 99.968, 99.956, 99.968, 99.996, 99.964, 99.988, 99.936, 99.964, 99.952, 98.12000001464844, 99.944, 99.984, 99.984, 99.984, 99.98, 99.972, 99.424, 99.972, 99.964, 99.972, 99.98, 99.964, 99.98, 91.00799999023438]\n",
            "Validation accuracies: [74.56, 85.57, 88.93, 70.6, 85.41, 88.89, 88.5, 86.11, 84.08, 84.21, 71.96, 88.23, 90.25, 85.36, 78.78, 85.47, 78.99, 83.3, 89.24, 87.51, 71.41, 87.75, 85.98, 87.97, 86.52, 80.38, 89.06, 88.43, 85.09, 33.93, 87.73, 90.29, 88.62, 86.57, 89.02, 89.79, 89.79, 88.96, 87.03, 86.9, 88.97, 89.79, 90.74, 89.89, 89.81, 89.75, 89.92, 89.79, 89.93, 89.84, 88.76, 90.07, 89.99, 89.81, 90.36, 89.75, 89.79, 89.79, 90.6, 89.95, 89.41, 89.64, 90.36, 88.88, 90.58, 89.81, 90.33, 85.94, 90.28, 90.2, 89.58, 89.4, 89.79, 89.76, 90.4, 90.98, 90.25, 90.4, 90.65, 90.4, 90.29, 90.67, 88.43, 90.75, 88.89, 86.72, 89.66, 90.67, 89.86, 89.86, 90.51, 90.15, 86.75, 90.15, 88.45, 89.29, 90.51, 88.45, 90.65, 84.22]\n",
            "\u001b[32m[10/06 13:43:50 nl.defaults.trainer]: \u001b[0mStart evaluation\n",
            "\u001b[32m[10/06 13:43:50 nl.defaults.trainer]: \u001b[0mloading model from file runs/RegularizedEvolution/NasBench201SearchSpace/cifar10/9001/search/model_final.pth\n",
            "\u001b[32m[10/06 13:43:50 nl.defaults.trainer]: \u001b[0mFinal architecture hash: (2, 2, 0, 3, 1, 2)\n",
            "\u001b[32m[10/06 13:43:50 nl.defaults.trainer]: \u001b[0mQueried results (Metric.VAL_ACCURACY): 90.98\n"
          ]
        }
      ],
      "source": [
        "# Set the optimizer and search space types\n",
        "# They will be instantiated inside run_optimizer\n",
        "optimizer_type = RegularizedEvolution # {RegularizedEvolution, RandomSearch}\n",
        "search_space_type = NasBench201SearchSpace # {NasBench101SearchSpace, NasBench201SearchSpace, NasBench301SearchSpace}\n",
        "\n",
        "# Set the dataset\n",
        "dataset = 'cifar10' # cifar10 for NB101 and NB301, {cifar100, ImageNet16-120} for NB201\n",
        "\n",
        "# The configuration used by the Trainer and Optimizer\n",
        "# The missing information will be populated inside run_optimizer\n",
        "config = {\n",
        "    'search': {\n",
        "        # Required by Trainer\n",
        "        'epochs': 100,\n",
        "        'checkpoint_freq': 100,\n",
        "        \n",
        "        # Required by Random Search optimizer\n",
        "        'fidelity': -1,\n",
        "        \n",
        "        # Required by RegularizedEvolution\n",
        "        'sample_size': 10,\n",
        "        'population_size': 30,\n",
        "    }\n",
        "}\n",
        "config = CfgNode.load_cfg(json.dumps(config))\n",
        "\n",
        "search_trajectory, best_model, best_model_val_acc = run_optimizer(\n",
        "                                                        optimizer_type,\n",
        "                                                        search_space_type,\n",
        "                                                        dataset,\n",
        "                                                        dataset_api,\n",
        "                                                        config,\n",
        "                                                        9001\n",
        "                                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulxKHDeKiQ7S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mvenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13 (default, Mar 28 2022, 11:38:47) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3a138bae05203fa8eb4bf07493da4bb9038fdb3e1f2f10b7ab3cd8c9223b9122"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
